{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/30 16:25:29 WARN Utils: Your hostname, DESKTOP-9EJISJ5 resolves to a loopback address: 127.0.1.1; using 172.18.237.79 instead (on interface eth0)\n",
      "22/07/30 16:25:29 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/30 16:25:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.app.submitTime', '1659191130614'),\n",
       " ('spark.executor.memory', '10g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.executor.cores', '2'),\n",
       " ('spark.driver.memory', '5g'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.kryoserializer.buffer.max', '1g'),\n",
       " ('spark.app.startTime', '1659191132452'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.app.id', 'local-1659191132500'),\n",
       " ('spark.driver.port', '33443'),\n",
       " ('spark.executor.extraJavaOptions',\n",
       "  '-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.cores.max', '2'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '172.18.237.79'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# import pyspark\n",
    "# sc = pyspark.SparkContext()\n",
    "# conf = pyspark.SparkConf().setAll([('spark.executor.memory', '10g'), ('spark.executor.cores', '2'),\n",
    "#     ('spark.cores.max', '2'), ('spark.driver.memory','5g'), (\"spark.kryoserializer.buffer.max\",\"1g\"),\n",
    "#     (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")])\n",
    "# sc.stop()\n",
    "# sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "ss.sparkContext.setLogLevel(\"ERROR\")\n",
    "ss.sparkContext.getConf().getAll()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DATA_REES46 = \"../data/raw/rees46/_/\"\n",
    "DATA_RR = \"../data/raw/retailrocket/_/\"\n",
    "events = ss.read.parquet(DATA_RR+\"events\", header=True, inferSchema=True, nanValue=\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|            min_time|            max_time|\n",
      "+--------------------+--------------------+\n",
      "|2015-05-09 22:00:...|2015-09-17 21:59:...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n interactions\n",
    "n_interactions = events.count()\n",
    "n_transactions = events.where(f.col(\"purchase\")==1).agg(f.countDistinct(\"user_session_id\")).toPandas().values[0,0]\n",
    "avg_revenue = events.where(f.col(\"purchase\")==1).groupby(\"user_session_id\")\\\n",
    "    .agg(f.sum(f.col(\"price\")).alias(\"revenue\")).agg(f.mean(f.col(\"revenue\")).alias(\"avg_rev\")).toPandas().values[0,0]\n",
    "n_users = events.select(\"user_id\").distinct().count()\n",
    "n_customers = events.where(f.col(\"purchase\")==1).select(\"user_id\").distinct().count() \n",
    "# time range\n",
    "events.agg(f.min(f.col(\"event_time\")).alias(\"min_time\"), f.max(f.col(\"event_time\")).alias(\"max_time\")).show()  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
