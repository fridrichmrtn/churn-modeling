{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Campaign profit estimation\n",
    "\n",
    "#### Expected campaign profit\n",
    "\n",
    "The classical approach to retention management is to identify the customers at high risk of churning and persuade them to stay active with promotional offers. Predictive solutions are, however, primarily evaluated in terms of the classification task, not in the retention management context. To reflect on those additional steps, we propose an evaluation procedure inspired by the retention campaign (Neslin et al., 2006) and generated profit frameworks (Tamaddoni et al., 2015).\n",
    "However, we focus on  development of customer value using conservative $CAP$ perspective, which allows us to differentiate customer profit on individual level and indicate changes in customer behavior. We also allow for differentiation of the parameters across the customer base and include uncertainty in the analysis.\n",
    "\n",
    "If individual customer $i$ is included in retention campaign, we expect its profit to amount to\n",
    "\n",
    "$$ \\pi_i^{expected} = p_i[\\gamma_i (CAP_i-\\delta)] + (1-p_i)[-\\psi_i \\delta]$$\n",
    "\n",
    "where $p_i$ is estimated probability churning, $\\gamma_i$ is probability of customer being convinced by retention offer to stay, $\\psi_i$ is probability of staying customer to accept the retention offer, and $\\delta$ is cost of the retention offer.\n",
    "As a result, we are able to estimate expected profit/loss of including said customer into the retention campaign using the probability of churning $p_i$.\n",
    "For retention campaign, we simply calculate the expected profit as\n",
    "\n",
    "$$\\Pi^{expected} = \\sum{\\pi_i^{expected}}$$\n",
    "\n",
    "where $\\pi_i$ represents the expected profit from including customer $i$ in the retention activity. Now, we sort customers with respect to the $\\pi_i$ and consider only customers that positively contribute to the $\\Pi$.\n",
    "In other words, we rank customers from highest to lowest $\\pi_i$ and construct the cumulative profit curve and find its maximum. Thus, we calculate both maximum estimated campaign profit and campaign size.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Actual campaign profit\n",
    "\n",
    "Over the previous paragraphs, we sum up how to use predictive solutions to target right customers. The approach however does not reflect on actual predictive performace.\n",
    "To address that, we extend on previous equations with target labels in mind. If individual customer $i$ is included in retention campaign, we calculate the actual profit as\n",
    "\n",
    "$$ \\pi_i^{actual} = y[\\gamma_i(CAP_i-\\delta)]+(1-y)[-\\psi_i\\delta]$$\n",
    "\n",
    "where $y$ denotes binary target variable. For the whole retention campaign, we simply calculate the expected profit as\n",
    "\n",
    "$$\\Pi^{actual} = \\sum{\\pi_i^{actual}}$$\n",
    "\n",
    "Please note that we use this approach for paper testing (campaign not realized).\n",
    "\n",
    "#### Simulation procedure\n",
    "\n",
    "As $\\gamma_i$ and $\\psi_i$ are unknown to us, we estimate their impact using simulation procedure with 1000 draws. Similarly to Tamaddoni et al. (2015), we expect them to follow distributions with\n",
    "$ \\gamma_i = Beta(20.5, 116.1) $, $E(\\gamma_i)=0.15$ and $ \\psi_i = Beta(9, 1) $ with $E(\\psi_i)=0.9$.\n",
    "This allows us to include heterogenous redemption rates and uncertainty into our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2:0.092\n",
      "mae:0.189\n",
      "mse:0.102\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "##\n",
    "### LOAD DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "data = _optimize_numeric_dtypes(\n",
    "    pd.read_parquet(\"../data/customer_model/retailrocket/\"))\n",
    "\n",
    "data = data.sort_values(\"time_step\", ascending=False)\n",
    "\n",
    "out_cols = [\"user_id\", \"row_id\", \"time_step\",\"target_event\",\n",
    "       \"target_revenue\", \"target_customer_value\", \"time_step\",\n",
    "       \"target_customer_value_lag1\", \"target_actual_profit\"]\n",
    "feat_cols = [c for c in data.columns if c not in set(out_cols)]\n",
    "target_cols = [\"target_event\"]\n",
    "\n",
    "trf = data.time_step>2\n",
    "tef = data.time_step==2\n",
    "\n",
    "X_train, y_train = data.loc[trf,feat_cols], data.loc[trf,target_cols]\n",
    "X_test, y_test = data.loc[tef,feat_cols], data.loc[tef,target_cols]\n",
    "\n",
    "#\n",
    "##\n",
    "### FIT CLASSIFIER\n",
    "\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer, PowerTransformer, RobustScaler, StandardScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "lgbm = Pipeline(\n",
    "    [(\"data_scaler\", StandardScaler()),\n",
    "    (\"rf\",TransformedTargetRegressor(regressor=RandomForestRegressor(), transformer=None))])\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "#\n",
    "##\n",
    "### EVALUATE USING STANDARD METRICS\n",
    "\n",
    "y_pred = lgbm.predict(X_test)\n",
    "#y_pred_proba = lgbm.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"r2:{:.3f}\".format(r2_score(y_test, y_pred)))\n",
    "print(\"mae:{:.3f}\".format(mean_absolute_error(y_test, y_pred)))\n",
    "print(\"mse:{:.3f}\".format(mean_squared_error(y_test, y_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('data_scaler', StandardScaler()),\n",
       "  ('rf', TransformedTargetRegressor(regressor=RandomForestRegressor()))],\n",
       " 'verbose': False,\n",
       " 'data_scaler': StandardScaler(),\n",
       " 'rf': TransformedTargetRegressor(regressor=RandomForestRegressor()),\n",
       " 'data_scaler__copy': True,\n",
       " 'data_scaler__with_mean': True,\n",
       " 'data_scaler__with_std': True,\n",
       " 'rf__check_inverse': True,\n",
       " 'rf__func': None,\n",
       " 'rf__inverse_func': None,\n",
       " 'rf__regressor__bootstrap': True,\n",
       " 'rf__regressor__ccp_alpha': 0.0,\n",
       " 'rf__regressor__criterion': 'squared_error',\n",
       " 'rf__regressor__max_depth': None,\n",
       " 'rf__regressor__max_features': 1.0,\n",
       " 'rf__regressor__max_leaf_nodes': None,\n",
       " 'rf__regressor__max_samples': None,\n",
       " 'rf__regressor__min_impurity_decrease': 0.0,\n",
       " 'rf__regressor__min_samples_leaf': 1,\n",
       " 'rf__regressor__min_samples_split': 2,\n",
       " 'rf__regressor__min_weight_fraction_leaf': 0.0,\n",
       " 'rf__regressor__n_estimators': 100,\n",
       " 'rf__regressor__n_jobs': None,\n",
       " 'rf__regressor__oob_score': False,\n",
       " 'rf__regressor__random_state': None,\n",
       " 'rf__regressor__verbose': 0,\n",
       " 'rf__regressor__warm_start': False,\n",
       " 'rf__regressor': RandomForestRegressor(),\n",
       " 'rf__transformer': None}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "###  EVALUATE USING PROFIT METRICS\n",
    "\n",
    "df = data.loc[tef,[\"user_id\",\"target_event\", \"target_cap\", \"prev_target_cap\"]]\n",
    "df[\"y_pred_proba\"] = y_pred_proba\n",
    "\n",
    "# NOTE: JUSTIFY THE CONFIG\n",
    "config = {\n",
    "    \"gamma\":{\"alpha\":2.04, \"beta\":202.04},\n",
    "    \"delta\":7500, \n",
    "    \"psi\":{\"alpha\":6.12, \"beta\":3.15},\n",
    "    \"n_iter\":1000,\n",
    "    \"seed\":1}\n",
    "\n",
    "gamma = config[\"gamma\"]\n",
    "delta = config[\"delta\"]\n",
    "psi = config[\"psi\"]\n",
    "n_iter = config[\"n_iter\"]\n",
    "seed = config[\"seed\"]    \n",
    "\n",
    "n_users = df.user_id.nunique()\n",
    "sp = []\n",
    "for i in range(n_iter):\n",
    "    gamma_psi = pd.DataFrame.from_dict({\n",
    "        \"user_id\":df.user_id.unique(),\n",
    "        \"gamma\":np.random.beta(gamma[\"alpha\"], gamma[\"beta\"], size=n_users),\n",
    "        \"psi\":np.random.beta(psi[\"alpha\"], psi[\"beta\"], size=n_users)})\n",
    "    temp = df.merge(gamma_psi, on=[\"user_id\"])\n",
    "    temp[\"ecp\"] = (temp[\"y_pred_proba\"]*temp[\"gamma\"]*(temp[\"prev_target_cap\"]-delta)\n",
    "        + (1-temp[\"y_pred_proba\"])*(-temp[\"psi\"]*delta))\n",
    "    temp[\"acp\"] = (temp[\"target_event\"]*temp[\"gamma\"]*(temp[\"target_cap\"]-delta)\n",
    "        + (1-temp[\"target_event\"])*(-temp[\"psi\"]*delta))\n",
    "    sp.append(temp.loc[:,[\"user_id\",\"ecp\", \"acp\", \"target_event\", \"y_pred_proba\" ,\"target_cap\"]])\n",
    "sp = pd.concat(sp)\n",
    "sp = sp.groupby([\"user_id\"], as_index=False).mean().sort_values(\"ecp\", ascending=False)\n",
    "sp[\"cecp\"] = sp.ecp.cumsum()\n",
    "sp[\"perc\"] = sp.ecp.rank(ascending=False, pct=True) \n",
    "sp[\"cacp\"] = sp.acp.cumsum()\n",
    "\n",
    "print(\"campaign size: {:.3f}\".format(sp.perc[sp.cecp.idxmax()]))\n",
    "print(\"expected campaign profit: {:.2f}\".format( sp.cecp.max()))\n",
    "print(\"actual campaign profit: {:.2f}\".format(sp.cacp[sp.cecp.idxmax()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the plot below, we can see the cumulative sum of campaign expected profit and campaign actual profit with respect to the percentiles., ranked according to the former one. We pick that dimension as it is related to the actual ranking procedure and the campaign size. As a result, we are able to diagnose the classifiers\" ranking abilities, unlike Tamaddoni et al. (2015).\n",
    "We see that the procedure struggles in the middle part of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "def plot_simulated_profit(sp):    \n",
    "    f, a = plt.subplots(1,1, figsize=(10,7))\n",
    "    sns.lineplot(#data=sp,\n",
    "        x=sp.perc, y=sp.cecp, legend=False,\n",
    "        color=sns.color_palette(\"rocket\")[0], ax=a);\n",
    "    sns.lineplot(#data=sp,\n",
    "        x=sp.perc, y=sp.cacp, legend=False,\n",
    "        color=sns.color_palette(\"rocket\")[3], ax=a);\n",
    "    a.set_ylabel(\"profit\");\n",
    "    a.set_xlabel(\"percentile\");\n",
    "    a.legend(loc=\"lower left\",\n",
    "        labels=[\"expected profit\", \"actual profit\"]);\n",
    "    a.axhline(0, linestyle=\"dotted\", c=\"k\");\n",
    "    return None\n",
    "plot_simulated_profit(sp)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
