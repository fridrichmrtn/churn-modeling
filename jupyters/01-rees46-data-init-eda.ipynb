{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = \"../data/raw/rees46/\"\n",
    "DATA_OUT = \"../data/intermediate/rees46/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# later on - probably add dtypes in load phase\n",
    "from pyspark.sql.types import StringType, IntegerType,\\\n",
    "    DoubleType, StructType, StructField, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/15 17:25:05 WARN Utils: Your hostname, mf-H110M-S2H resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface enx00d2b176ce73)\n",
      "22/03/15 17:25:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/15 17:25:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "#sc = pyspark.SparkContext()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "# terminate # set parames # get or create\n",
    "ss.sparkContext.setLogLevel(\"ERROR\")\n",
    "ss.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "#ss.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_info(sdf, name=\"\"):\n",
    "    rc = sdf.count(); cc = len(sdf.columns) # note - wrap this into a fuc\n",
    "    print(\"The {} dataset has shape of ({},{}) and following dtypes\\n{}\".\\\n",
    "        format(name, rc, cc, sdf.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (67542878,9) and following dtypes\n",
      "[('event_time', 'string'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_id', 'int'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for f in glob.glob(DATA_IN+\"*.gz\"):\n",
    "    if \"events\" not in locals():\n",
    "        events = ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\")\n",
    "    else:\n",
    "        events.union(ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\"))\n",
    "#events = ss.read.csv(glob.glob(DATA_DIR+\"*.gz\")[2],header=True, inferSchema=True)\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:=================================================>    (183 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (25219436,9) and following dtypes\n",
      "[('user_id', 'int'), ('event_time', 'string'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keep interactions for users with >=10 trans\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "users_to_keep = events.filter(col(\"event_type\")==\"purchase\").\\\n",
    "    groupBy(\"user_id\").agg(countDistinct(\"user_session\").\\\n",
    "        alias(\"n_transactions\")).filter(col(\"n_transactions\")>=0).select(col(\"user_id\"))\n",
    "events = events.join(users_to_keep, [\"user_id\"], \"inner\")\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===================================================>  (189 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (25219436,9) and following dtypes\n",
      "[('user_id', 'int'), ('event_time', 'timestamp'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert dates\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "events = events.withColumn(\"event_time\", to_timestamp(col(\"event_time\")))\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The products dataset has shape of (180907,4) and following dtypes\n",
      "[('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 35:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The events dataset has shape of (25219436,6) and following dtypes\n",
      "[('event_time', 'timestamp'), ('event_type', 'string'), ('user_id', 'int'), ('product_id', 'int'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# carve-out product-cat info as it is duplicated\n",
    "products = events.select([\"product_id\", \"category_id\", \"category_code\", \"brand\"]).dropDuplicates()\n",
    "events = events.select([\"event_time\", \"event_type\", \"user_id\", \"product_id\", \"price\",\"user_session\"])\n",
    "get_sdf_info(products, \"products\")\n",
    "get_sdf_info(events, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# try to save both of the datasets to gz\n",
    "import os\n",
    "if not os.path.exists(DATA_OUT):\n",
    "    os.makedirs(DATA_OUT)\n",
    "events.write.csv(DATA_OUT+\"events\", compression=\"gzip\")\n",
    "products.write.csv(DATA_OUT+\"products\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f541d2ce529b630cdc96c7c32eb4a5a175871233e2dcc03350c8ccd9d2e580"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
