{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### LOAD DATA\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "data = _optimize_numeric_dtypes(\n",
    "    pd.read_parquet(\"../data/customer_model/retailrocket/\"))\n",
    "#data[\"target_cap\"] = data[\"target_cap\"].clip(-1000,10000)\n",
    "\n",
    "#\n",
    "##\n",
    "### CONSTRUCT PROFIT TARGET\n",
    "# NOTE: encapsulate this\n",
    "config = {\n",
    "    \"gamma\":{\"alpha\":2.04, \"beta\":202.04},\n",
    "    \"delta\":7500, \n",
    "    \"psi\":{\"alpha\":6.12, \"beta\":3.15},\n",
    "    \"n_iter\":1000,\n",
    "    \"seed\":1}\n",
    "    \n",
    "gamma = config[\"gamma\"]\n",
    "delta = config[\"delta\"]\n",
    "psi = config[\"psi\"]\n",
    "n_iter = config[\"n_iter\"]\n",
    "seed = config[\"seed\"] \n",
    "\n",
    "n_users = data.user_id.nunique()\n",
    "sp = []\n",
    "for i in range(n_iter):\n",
    "    gamma_psi = pd.DataFrame.from_dict({\n",
    "        \"user_id\":data.user_id.unique(),\n",
    "        \"gamma\":np.random.beta(gamma[\"alpha\"], gamma[\"beta\"], size=n_users),\n",
    "        \"psi\":np.random.beta(psi[\"alpha\"], psi[\"beta\"], size=n_users)})\n",
    "    temp = data.merge(gamma_psi, on=[\"user_id\"])\n",
    "    temp[\"acp\"] = (temp[\"target_event\"]*temp[\"gamma\"]*(temp[\"target_cap\"]-delta)\n",
    "        + (1-temp[\"target_event\"])*(-temp[\"psi\"]*delta))\n",
    "    sp.append(temp.loc[:,[\"user_id\", \"week_step\", \"acp\"]])\n",
    "sp = pd.concat(sp)\n",
    "\n",
    "#\n",
    "##\n",
    "### PUT IT TOGETHER\n",
    "\n",
    "out_cols = [\"user_id\", \"row_id\", \"target_event\",\n",
    "    \"target_revenue\", \"week_step\", \"target_cap\", \"cap\"]\n",
    "feat_cols = [c for c in data.columns if c not in set(out_cols)]\n",
    "target_cols = [\"target_event\",\"acp\"]\n",
    "data = data.merge(sp.groupby([\"user_id\",\"week_step\"], as_index=False).acp.mean(), on=[\"user_id\",\"week_step\"])\n",
    "\n",
    "\n",
    "trf = data.week_step>5\n",
    "tef = data.week_step==5\n",
    "X_train, y_train = data.loc[trf,feat_cols].values, data.loc[trf,target_cols].values\n",
    "X_test, y_test = data.loc[tef,feat_cols].values, data.loc[tef,target_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-14 18:11:26.334818: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-14 18:11:26.334868: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "##\n",
    "### COMBINED NEURAL NETWORK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "\n",
    "class MultiOutputTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "\n",
    "        self.class_encoder_ = OneHotEncoder(sparse=False)\n",
    "        self.reg_transformer_ = PowerTransformer()\n",
    "        # Fit them to the input data\n",
    "        self.class_encoder_.fit(y_class)\n",
    "        self.reg_transformer_.fit(y_reg)\n",
    "        # Save the number of classes\n",
    "        self.n_classes_ = len(self.class_encoder_.categories_)\n",
    "        self.n_outputs_expected_ = 2\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "        # Apply transformers to input array\n",
    "        y_class = self.class_encoder_.transform(y_class)\n",
    "        y_reg = self.reg_transformer_.transform(y_reg)\n",
    "        # Split the data into a list\n",
    "        return [y_class, y_reg]\n",
    "\n",
    "    def inverse_transform(self, y, return_proba=False):\n",
    "        y_pred_reg = y[1]\n",
    "        if return_proba:\n",
    "            return y[0]\n",
    "        else:\n",
    "            y_pred_class = np.zeros_like(y[0])\n",
    "            y_pred_class[np.arange(len(y[0])), np.argmax(y[0], axis=1)] = 1\n",
    "            y_pred_class = self.class_encoder_.inverse_transform(y_pred_class)\n",
    "        y_pred_reg = self.reg_transformer_.inverse_transform(y_pred_reg)\n",
    "        return np.column_stack([y_pred_class, y_pred_reg])\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return {\n",
    "            \"n_classes_\": self.n_classes_,\n",
    "            \"n_outputs_expected_\": self.n_outputs_expected_,\n",
    "        }\n",
    "\n",
    "from scikeras.wrappers import BaseWrapper\n",
    "from tensorflow.keras.initializers import HeNormal, LecunNormal, HeNormal\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, concatenate, LeakyReLU\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class CombiNet(BaseWrapper):\n",
    "\n",
    "    def __init__(self, activation = \"selu\",\n",
    "        se_layers=1, se_units=256,\n",
    "        re_layers=5, re_units=100,\n",
    "        ce_layers=5, ce_units=100, cc_units=75,\n",
    "        epochs=10, verbose=0,\n",
    "        optimizer=\"adam\", optimizer__clipvalue=1.0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.activation = activation\n",
    "            self.se_layers = se_layers\n",
    "            self.se_units = se_units\n",
    "            self.re_layers = re_layers\n",
    "            self.re_units = re_units\n",
    "            self.ce_layers = ce_layers\n",
    "            self.ce_units = ce_units\n",
    "            self.cc_units = cc_units\n",
    "            self.epochs = epochs\n",
    "            self.verbose = verbose\n",
    "            self.prediction_scope_ = {\"classification\":0,\"regression\":1,\"full\":range(2)}\n",
    "\n",
    "    def _get_weight_init(self):\n",
    "        if isinstance(self.activation, LeakyReLU):\n",
    "            \n",
    "            init = HeNormal()\n",
    "        elif self.activation in [\"selu\", \"elu\"]:\n",
    "            init = LecunNormal()\n",
    "        else:\n",
    "            init = HeNormal()  \n",
    "        return init\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs):\n",
    "        weight_init = self._get_weight_init()\n",
    "\n",
    "        # shared extraction\n",
    "        inp = Input(shape=(self.n_features_in_))\n",
    "        fe = inp\n",
    "        for i in range(self.se_layers):\n",
    "            fe = Dense(self.se_units, self.activation,\n",
    "                kernel_initializer=weight_init)(fe)\n",
    "            fe = BatchNormalization()(fe)\n",
    "        # regression branch\n",
    "        re = fe\n",
    "        for i in range(self.re_layers):\n",
    "            re = Dense(self.re_units, self.activation,\n",
    "                kernel_initializer=weight_init)(re)\n",
    "            re = BatchNormalization()(re)\n",
    "        rr_head = Dense(1,\"linear\")(re)\n",
    "        # classification branch\n",
    "        ce = fe\n",
    "        for i in range(self.ce_layers):\n",
    "            ce = Dense(self.ce_units, self.activation,\n",
    "                kernel_initializer=weight_init)(ce)\n",
    "            ce = BatchNormalization()(ce)\n",
    "        cc = Dense(self.cc_units, self.activation,\n",
    "            kernel_initializer=weight_init)(concatenate([ce, re]))\n",
    "        cc = BatchNormalization()(cc)\n",
    "        cc_head = Dense(2, \"softmax\")(cc)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=[cc_head, rr_head])\n",
    "        model.compile(loss=[\"categorical_crossentropy\",\"mse\"], loss_weights=[.5,.5],\n",
    "            optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model\n",
    "        \n",
    "    @property\n",
    "    def target_encoder(self):\n",
    "        return MultiOutputTransformer()\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        return self.target_encoder_.inverse_transform(y_pred, return_proba=True)\n",
    "\n",
    "    def predict(self, X, scope=\"classification\"):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        y_pred = self.target_encoder_.inverse_transform(y_pred)\n",
    "        return y_pred[:,self.prediction_scope_[scope]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "67/67 [==============================] - 2s 6ms/step - loss: 1.2741 - dense_63_loss: 0.6921 - dense_60_loss: 1.8561\n",
      "Epoch 2/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.7415 - dense_63_loss: 0.5745 - dense_60_loss: 0.9084\n",
      "Epoch 3/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6739 - dense_63_loss: 0.5504 - dense_60_loss: 0.7973\n",
      "Epoch 4/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6497 - dense_63_loss: 0.5271 - dense_60_loss: 0.7724\n",
      "Epoch 5/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6389 - dense_63_loss: 0.5254 - dense_60_loss: 0.7524\n",
      "Epoch 6/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.6067 - dense_63_loss: 0.5045 - dense_60_loss: 0.7089\n",
      "Epoch 7/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6290 - dense_63_loss: 0.5026 - dense_60_loss: 0.7554\n",
      "Epoch 8/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6109 - dense_63_loss: 0.5070 - dense_60_loss: 0.7149\n",
      "Epoch 9/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5914 - dense_63_loss: 0.4949 - dense_60_loss: 0.6880\n",
      "Epoch 10/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6018 - dense_63_loss: 0.5003 - dense_60_loss: 0.7032\n",
      "Epoch 11/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.6096 - dense_63_loss: 0.4920 - dense_60_loss: 0.7272\n",
      "Epoch 12/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5582 - dense_63_loss: 0.4691 - dense_60_loss: 0.6473\n",
      "Epoch 13/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5732 - dense_63_loss: 0.4807 - dense_60_loss: 0.6657\n",
      "Epoch 14/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5919 - dense_63_loss: 0.4719 - dense_60_loss: 0.7119\n",
      "Epoch 15/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5925 - dense_63_loss: 0.4763 - dense_60_loss: 0.7087\n",
      "Epoch 16/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5848 - dense_63_loss: 0.4692 - dense_60_loss: 0.7005\n",
      "Epoch 17/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5571 - dense_63_loss: 0.4553 - dense_60_loss: 0.6590\n",
      "Epoch 18/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5601 - dense_63_loss: 0.4632 - dense_60_loss: 0.6569\n",
      "Epoch 19/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5496 - dense_63_loss: 0.4615 - dense_60_loss: 0.6376\n",
      "Epoch 20/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5513 - dense_63_loss: 0.4579 - dense_60_loss: 0.6447\n",
      "Epoch 21/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5285 - dense_63_loss: 0.4408 - dense_60_loss: 0.6163\n",
      "Epoch 22/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5452 - dense_63_loss: 0.4436 - dense_60_loss: 0.6467\n",
      "Epoch 23/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5230 - dense_63_loss: 0.4447 - dense_60_loss: 0.6013\n",
      "Epoch 24/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.5354 - dense_63_loss: 0.4377 - dense_60_loss: 0.6331\n",
      "Epoch 25/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5243 - dense_63_loss: 0.4292 - dense_60_loss: 0.6193\n",
      "Epoch 26/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5361 - dense_63_loss: 0.4460 - dense_60_loss: 0.6262\n",
      "Epoch 27/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5325 - dense_63_loss: 0.4365 - dense_60_loss: 0.6284\n",
      "Epoch 28/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5403 - dense_63_loss: 0.4485 - dense_60_loss: 0.6322\n",
      "Epoch 29/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4974 - dense_63_loss: 0.4241 - dense_60_loss: 0.5707\n",
      "Epoch 30/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5127 - dense_63_loss: 0.4239 - dense_60_loss: 0.6014\n",
      "Epoch 31/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5314 - dense_63_loss: 0.4313 - dense_60_loss: 0.6314\n",
      "Epoch 32/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4977 - dense_63_loss: 0.4279 - dense_60_loss: 0.5675\n",
      "Epoch 33/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.5044 - dense_63_loss: 0.4196 - dense_60_loss: 0.5893\n",
      "Epoch 34/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5084 - dense_63_loss: 0.4267 - dense_60_loss: 0.5901\n",
      "Epoch 35/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4958 - dense_63_loss: 0.4247 - dense_60_loss: 0.5670\n",
      "Epoch 36/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4863 - dense_63_loss: 0.4100 - dense_60_loss: 0.5627\n",
      "Epoch 37/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4936 - dense_63_loss: 0.4152 - dense_60_loss: 0.5720\n",
      "Epoch 38/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4788 - dense_63_loss: 0.4057 - dense_60_loss: 0.5519\n",
      "Epoch 39/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.5001 - dense_63_loss: 0.4079 - dense_60_loss: 0.5923\n",
      "Epoch 40/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4898 - dense_63_loss: 0.4130 - dense_60_loss: 0.5666\n",
      "Epoch 41/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4856 - dense_63_loss: 0.4114 - dense_60_loss: 0.5598\n",
      "Epoch 42/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4913 - dense_63_loss: 0.4203 - dense_60_loss: 0.5623\n",
      "Epoch 43/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4856 - dense_63_loss: 0.4103 - dense_60_loss: 0.5609\n",
      "Epoch 44/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4644 - dense_63_loss: 0.3844 - dense_60_loss: 0.5444\n",
      "Epoch 45/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4763 - dense_63_loss: 0.4119 - dense_60_loss: 0.5407\n",
      "Epoch 46/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4551 - dense_63_loss: 0.3954 - dense_60_loss: 0.5148\n",
      "Epoch 47/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4948 - dense_63_loss: 0.4098 - dense_60_loss: 0.5798\n",
      "Epoch 48/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4625 - dense_63_loss: 0.4015 - dense_60_loss: 0.5235\n",
      "Epoch 49/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4851 - dense_63_loss: 0.4003 - dense_60_loss: 0.5699\n",
      "Epoch 50/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4838 - dense_63_loss: 0.4091 - dense_60_loss: 0.5585\n",
      "Epoch 51/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4921 - dense_63_loss: 0.4124 - dense_60_loss: 0.5718\n",
      "Epoch 52/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4746 - dense_63_loss: 0.4024 - dense_60_loss: 0.5467\n",
      "Epoch 53/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4637 - dense_63_loss: 0.3860 - dense_60_loss: 0.5414\n",
      "Epoch 54/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4649 - dense_63_loss: 0.3949 - dense_60_loss: 0.5350\n",
      "Epoch 55/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4495 - dense_63_loss: 0.3774 - dense_60_loss: 0.5216\n",
      "Epoch 56/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4612 - dense_63_loss: 0.3880 - dense_60_loss: 0.5344\n",
      "Epoch 57/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4612 - dense_63_loss: 0.3994 - dense_60_loss: 0.5230\n",
      "Epoch 58/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4337 - dense_63_loss: 0.3761 - dense_60_loss: 0.4913\n",
      "Epoch 59/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4490 - dense_63_loss: 0.3917 - dense_60_loss: 0.5063\n",
      "Epoch 60/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4537 - dense_63_loss: 0.3953 - dense_60_loss: 0.5122\n",
      "Epoch 61/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4507 - dense_63_loss: 0.3686 - dense_60_loss: 0.5328\n",
      "Epoch 62/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4432 - dense_63_loss: 0.3678 - dense_60_loss: 0.5186\n",
      "Epoch 63/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4707 - dense_63_loss: 0.3961 - dense_60_loss: 0.5454\n",
      "Epoch 64/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4337 - dense_63_loss: 0.3839 - dense_60_loss: 0.4834\n",
      "Epoch 65/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4507 - dense_63_loss: 0.3687 - dense_60_loss: 0.5327\n",
      "Epoch 66/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4487 - dense_63_loss: 0.3969 - dense_60_loss: 0.5005\n",
      "Epoch 67/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4434 - dense_63_loss: 0.3769 - dense_60_loss: 0.5100\n",
      "Epoch 68/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4317 - dense_63_loss: 0.3834 - dense_60_loss: 0.4801\n",
      "Epoch 69/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4249 - dense_63_loss: 0.3672 - dense_60_loss: 0.4826\n",
      "Epoch 70/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4156 - dense_63_loss: 0.3654 - dense_60_loss: 0.4658\n",
      "Epoch 71/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4325 - dense_63_loss: 0.3662 - dense_60_loss: 0.4988\n",
      "Epoch 72/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4505 - dense_63_loss: 0.3773 - dense_60_loss: 0.5237\n",
      "Epoch 73/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4452 - dense_63_loss: 0.3840 - dense_60_loss: 0.5064\n",
      "Epoch 74/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4405 - dense_63_loss: 0.3853 - dense_60_loss: 0.4957\n",
      "Epoch 75/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4067 - dense_63_loss: 0.3435 - dense_60_loss: 0.4698\n",
      "Epoch 76/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4371 - dense_63_loss: 0.3759 - dense_60_loss: 0.4982\n",
      "Epoch 77/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4061 - dense_63_loss: 0.3515 - dense_60_loss: 0.4607\n",
      "Epoch 78/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4092 - dense_63_loss: 0.3739 - dense_60_loss: 0.4446\n",
      "Epoch 79/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4060 - dense_63_loss: 0.3460 - dense_60_loss: 0.4659\n",
      "Epoch 80/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4348 - dense_63_loss: 0.3658 - dense_60_loss: 0.5037\n",
      "Epoch 81/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4089 - dense_63_loss: 0.3444 - dense_60_loss: 0.4733\n",
      "Epoch 82/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4053 - dense_63_loss: 0.3362 - dense_60_loss: 0.4744\n",
      "Epoch 83/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4035 - dense_63_loss: 0.3542 - dense_60_loss: 0.4529\n",
      "Epoch 84/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4032 - dense_63_loss: 0.3439 - dense_60_loss: 0.4625\n",
      "Epoch 85/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3933 - dense_63_loss: 0.3430 - dense_60_loss: 0.4437\n",
      "Epoch 86/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4135 - dense_63_loss: 0.3552 - dense_60_loss: 0.4719\n",
      "Epoch 87/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4120 - dense_63_loss: 0.3463 - dense_60_loss: 0.4778\n",
      "Epoch 88/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4253 - dense_63_loss: 0.3720 - dense_60_loss: 0.4787\n",
      "Epoch 89/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3741 - dense_63_loss: 0.3244 - dense_60_loss: 0.4239\n",
      "Epoch 90/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4312 - dense_63_loss: 0.3630 - dense_60_loss: 0.4994\n",
      "Epoch 91/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3963 - dense_63_loss: 0.3517 - dense_60_loss: 0.4410\n",
      "Epoch 92/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4134 - dense_63_loss: 0.3605 - dense_60_loss: 0.4662\n",
      "Epoch 93/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4148 - dense_63_loss: 0.3554 - dense_60_loss: 0.4741\n",
      "Epoch 94/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4066 - dense_63_loss: 0.3509 - dense_60_loss: 0.4623\n",
      "Epoch 95/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4092 - dense_63_loss: 0.3492 - dense_60_loss: 0.4692\n",
      "Epoch 96/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3895 - dense_63_loss: 0.3402 - dense_60_loss: 0.4387\n",
      "Epoch 97/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3958 - dense_63_loss: 0.3350 - dense_60_loss: 0.4566\n",
      "Epoch 98/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4228 - dense_63_loss: 0.3768 - dense_60_loss: 0.4689\n",
      "Epoch 99/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4042 - dense_63_loss: 0.3416 - dense_60_loss: 0.4668\n",
      "Epoch 100/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3934 - dense_63_loss: 0.3593 - dense_60_loss: 0.4275\n",
      "Epoch 101/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3792 - dense_63_loss: 0.3305 - dense_60_loss: 0.4279\n",
      "Epoch 102/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4308 - dense_63_loss: 0.3684 - dense_60_loss: 0.4932\n",
      "Epoch 103/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4123 - dense_63_loss: 0.3530 - dense_60_loss: 0.4716\n",
      "Epoch 104/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3893 - dense_63_loss: 0.3297 - dense_60_loss: 0.4488\n",
      "Epoch 105/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3984 - dense_63_loss: 0.3405 - dense_60_loss: 0.4563\n",
      "Epoch 106/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3816 - dense_63_loss: 0.3257 - dense_60_loss: 0.4376\n",
      "Epoch 107/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3929 - dense_63_loss: 0.3599 - dense_60_loss: 0.4259\n",
      "Epoch 108/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.4156 - dense_63_loss: 0.3733 - dense_60_loss: 0.4579\n",
      "Epoch 109/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3685 - dense_63_loss: 0.3264 - dense_60_loss: 0.4105\n",
      "Epoch 110/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3499 - dense_63_loss: 0.3243 - dense_60_loss: 0.3755\n",
      "Epoch 111/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3740 - dense_63_loss: 0.3364 - dense_60_loss: 0.4115\n",
      "Epoch 112/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3813 - dense_63_loss: 0.3490 - dense_60_loss: 0.4136\n",
      "Epoch 113/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3907 - dense_63_loss: 0.3344 - dense_60_loss: 0.4469\n",
      "Epoch 114/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3778 - dense_63_loss: 0.3255 - dense_60_loss: 0.4302\n",
      "Epoch 115/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3965 - dense_63_loss: 0.3511 - dense_60_loss: 0.4418\n",
      "Epoch 116/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3680 - dense_63_loss: 0.3341 - dense_60_loss: 0.4019\n",
      "Epoch 117/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3594 - dense_63_loss: 0.3230 - dense_60_loss: 0.3958\n",
      "Epoch 118/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3684 - dense_63_loss: 0.3282 - dense_60_loss: 0.4087\n",
      "Epoch 119/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3871 - dense_63_loss: 0.3302 - dense_60_loss: 0.4441\n",
      "Epoch 120/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3671 - dense_63_loss: 0.3369 - dense_60_loss: 0.3972\n",
      "Epoch 121/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3472 - dense_63_loss: 0.2995 - dense_60_loss: 0.3948\n",
      "Epoch 122/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3591 - dense_63_loss: 0.3125 - dense_60_loss: 0.4056\n",
      "Epoch 123/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3646 - dense_63_loss: 0.3291 - dense_60_loss: 0.4001\n",
      "Epoch 124/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3623 - dense_63_loss: 0.3319 - dense_60_loss: 0.3927\n",
      "Epoch 125/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3623 - dense_63_loss: 0.3363 - dense_60_loss: 0.3882\n",
      "Epoch 126/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3884 - dense_63_loss: 0.3385 - dense_60_loss: 0.4382\n",
      "Epoch 127/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3540 - dense_63_loss: 0.3237 - dense_60_loss: 0.3843\n",
      "Epoch 128/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3563 - dense_63_loss: 0.3371 - dense_60_loss: 0.3755\n",
      "Epoch 129/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3402 - dense_63_loss: 0.3113 - dense_60_loss: 0.3691\n",
      "Epoch 130/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3624 - dense_63_loss: 0.3296 - dense_60_loss: 0.3952\n",
      "Epoch 131/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3209 - dense_63_loss: 0.2869 - dense_60_loss: 0.3549\n",
      "Epoch 132/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3557 - dense_63_loss: 0.3151 - dense_60_loss: 0.3964\n",
      "Epoch 133/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3509 - dense_63_loss: 0.3053 - dense_60_loss: 0.3966\n",
      "Epoch 134/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3451 - dense_63_loss: 0.3039 - dense_60_loss: 0.3864\n",
      "Epoch 135/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3378 - dense_63_loss: 0.2970 - dense_60_loss: 0.3786\n",
      "Epoch 136/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3437 - dense_63_loss: 0.3119 - dense_60_loss: 0.3754\n",
      "Epoch 137/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3297 - dense_63_loss: 0.3087 - dense_60_loss: 0.3507\n",
      "Epoch 138/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3394 - dense_63_loss: 0.3060 - dense_60_loss: 0.3727\n",
      "Epoch 139/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3307 - dense_63_loss: 0.3060 - dense_60_loss: 0.3554\n",
      "Epoch 140/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3360 - dense_63_loss: 0.3083 - dense_60_loss: 0.3637\n",
      "Epoch 141/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.3361 - dense_63_loss: 0.2990 - dense_60_loss: 0.3732\n",
      "Epoch 142/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3643 - dense_63_loss: 0.3078 - dense_60_loss: 0.4208\n",
      "Epoch 143/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3405 - dense_63_loss: 0.3043 - dense_60_loss: 0.3767\n",
      "Epoch 144/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3389 - dense_63_loss: 0.2961 - dense_60_loss: 0.3816\n",
      "Epoch 145/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3553 - dense_63_loss: 0.3112 - dense_60_loss: 0.3994\n",
      "Epoch 146/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3230 - dense_63_loss: 0.3006 - dense_60_loss: 0.3455\n",
      "Epoch 147/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3132 - dense_63_loss: 0.2786 - dense_60_loss: 0.3477\n",
      "Epoch 148/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3185 - dense_63_loss: 0.2780 - dense_60_loss: 0.3590\n",
      "Epoch 149/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3424 - dense_63_loss: 0.3059 - dense_60_loss: 0.3789\n",
      "Epoch 150/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3280 - dense_63_loss: 0.2854 - dense_60_loss: 0.3705\n",
      "Epoch 151/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3541 - dense_63_loss: 0.3169 - dense_60_loss: 0.3912\n",
      "Epoch 152/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3238 - dense_63_loss: 0.3014 - dense_60_loss: 0.3461\n",
      "Epoch 153/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3410 - dense_63_loss: 0.3129 - dense_60_loss: 0.3691\n",
      "Epoch 154/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3381 - dense_63_loss: 0.2951 - dense_60_loss: 0.3812\n",
      "Epoch 155/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3289 - dense_63_loss: 0.2895 - dense_60_loss: 0.3683\n",
      "Epoch 156/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3159 - dense_63_loss: 0.2849 - dense_60_loss: 0.3469\n",
      "Epoch 157/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3555 - dense_63_loss: 0.3136 - dense_60_loss: 0.3974\n",
      "Epoch 158/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3567 - dense_63_loss: 0.3078 - dense_60_loss: 0.4057\n",
      "Epoch 159/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3275 - dense_63_loss: 0.3025 - dense_60_loss: 0.3525\n",
      "Epoch 160/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.2994 - dense_63_loss: 0.2774 - dense_60_loss: 0.3213\n",
      "Epoch 161/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3359 - dense_63_loss: 0.3182 - dense_60_loss: 0.3537\n",
      "Epoch 162/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3142 - dense_63_loss: 0.2874 - dense_60_loss: 0.3409\n",
      "Epoch 163/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3380 - dense_63_loss: 0.3080 - dense_60_loss: 0.3680\n",
      "Epoch 164/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3121 - dense_63_loss: 0.2829 - dense_60_loss: 0.3412\n",
      "Epoch 165/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3550 - dense_63_loss: 0.3060 - dense_60_loss: 0.4040\n",
      "Epoch 166/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3346 - dense_63_loss: 0.3032 - dense_60_loss: 0.3661\n",
      "Epoch 167/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3068 - dense_63_loss: 0.2901 - dense_60_loss: 0.3236\n",
      "Epoch 168/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3140 - dense_63_loss: 0.2887 - dense_60_loss: 0.3393\n",
      "Epoch 169/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3012 - dense_63_loss: 0.2776 - dense_60_loss: 0.3247\n",
      "Epoch 170/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3139 - dense_63_loss: 0.2767 - dense_60_loss: 0.3511\n",
      "Epoch 171/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2914 - dense_63_loss: 0.2620 - dense_60_loss: 0.3209\n",
      "Epoch 172/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3166 - dense_63_loss: 0.2971 - dense_60_loss: 0.3362\n",
      "Epoch 173/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2998 - dense_63_loss: 0.2750 - dense_60_loss: 0.3246\n",
      "Epoch 174/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3459 - dense_63_loss: 0.3153 - dense_60_loss: 0.3765\n",
      "Epoch 175/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3429 - dense_63_loss: 0.3004 - dense_60_loss: 0.3854\n",
      "Epoch 176/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3265 - dense_63_loss: 0.3004 - dense_60_loss: 0.3525\n",
      "Epoch 177/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3178 - dense_63_loss: 0.3008 - dense_60_loss: 0.3348\n",
      "Epoch 178/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3314 - dense_63_loss: 0.2988 - dense_60_loss: 0.3640\n",
      "Epoch 179/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3131 - dense_63_loss: 0.2820 - dense_60_loss: 0.3442\n",
      "Epoch 180/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2930 - dense_63_loss: 0.2691 - dense_60_loss: 0.3170\n",
      "Epoch 181/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3679 - dense_63_loss: 0.3303 - dense_60_loss: 0.4056\n",
      "Epoch 182/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3256 - dense_63_loss: 0.3023 - dense_60_loss: 0.3490\n",
      "Epoch 183/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3588 - dense_63_loss: 0.3175 - dense_60_loss: 0.4001\n",
      "Epoch 184/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3097 - dense_63_loss: 0.2872 - dense_60_loss: 0.3322\n",
      "Epoch 185/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3175 - dense_63_loss: 0.2871 - dense_60_loss: 0.3479\n",
      "Epoch 186/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3029 - dense_63_loss: 0.2761 - dense_60_loss: 0.3297\n",
      "Epoch 187/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3340 - dense_63_loss: 0.3003 - dense_60_loss: 0.3677\n",
      "Epoch 188/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3120 - dense_63_loss: 0.2752 - dense_60_loss: 0.3488\n",
      "Epoch 189/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3009 - dense_63_loss: 0.2607 - dense_60_loss: 0.3411\n",
      "Epoch 190/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2777 - dense_63_loss: 0.2575 - dense_60_loss: 0.2979\n",
      "Epoch 191/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2794 - dense_63_loss: 0.2649 - dense_60_loss: 0.2940\n",
      "Epoch 192/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2866 - dense_63_loss: 0.2782 - dense_60_loss: 0.2949\n",
      "Epoch 193/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2733 - dense_63_loss: 0.2463 - dense_60_loss: 0.3003\n",
      "Epoch 194/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2764 - dense_63_loss: 0.2623 - dense_60_loss: 0.2906\n",
      "Epoch 195/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2929 - dense_63_loss: 0.2603 - dense_60_loss: 0.3254\n",
      "Epoch 196/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2756 - dense_63_loss: 0.2492 - dense_60_loss: 0.3020\n",
      "Epoch 197/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3191 - dense_63_loss: 0.2816 - dense_60_loss: 0.3567\n",
      "Epoch 198/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3072 - dense_63_loss: 0.2700 - dense_60_loss: 0.3444\n",
      "Epoch 199/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2759 - dense_63_loss: 0.2506 - dense_60_loss: 0.3012\n",
      "Epoch 200/200\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.3086 - dense_63_loss: 0.2706 - dense_60_loss: 0.3467\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CombiNet(\n",
       "\tmodel=None\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=200\n",
       "\tactivation=selu\n",
       "\tse_layers=2\n",
       "\tse_units=512\n",
       "\tre_layers=1\n",
       "\tre_units=100\n",
       "\tce_layers=1\n",
       "\tce_units=100\n",
       "\tcc_units=75\n",
       ")</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CombiNet</label><div class=\"sk-toggleable__content\"><pre>CombiNet(\n",
       "\tmodel=None\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=200\n",
       "\tactivation=selu\n",
       "\tse_layers=2\n",
       "\tse_units=512\n",
       "\tre_layers=1\n",
       "\tre_units=100\n",
       "\tce_layers=1\n",
       "\tce_units=100\n",
       "\tcc_units=75\n",
       ")</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CombiNet(\n",
       "\tmodel=None\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=200\n",
       "\tactivation=selu\n",
       "\tse_layers=2\n",
       "\tse_units=512\n",
       "\tre_layers=1\n",
       "\tre_units=100\n",
       "\tce_layers=1\n",
       "\tce_units=100\n",
       "\tcc_units=75\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn = CombiNet(se_layers=2, se_units=512, batch_size=16,\n",
    "    re_layers=1, ce_layers=1, optimizer=\"adam\", epochs=200, verbose=1)\n",
    "\n",
    "cn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 0s 2ms/step\n",
      "r2:0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "y_pred  = cn.predict(X_train, scope=\"classification\")\n",
    "print(\"r2:{:.2f}\".format(f1_score(y_train[:,0], y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
