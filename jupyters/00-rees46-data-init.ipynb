{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = \"../data/raw/rees46/_\"\n",
    "DATA_OUT = \"../data/raw/rees46/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pyspark \n",
    "from pyspark.sql import SparkSession \n",
    "\n",
    "# later on - probably add dtypes in load phase\n",
    "from pyspark.sql.types import StringType, IntegerType,\\\n",
    "    DoubleType, StructType, StructField, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc = pyspark.SparkContext()\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "# terminate # set parames # get or create\n",
    "ss.sparkContext.setLogLevel(\"ERROR\")\n",
    "ss.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "#ss.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_info(sdf, name=\"\"):\n",
    "    rc = sdf.count(); cc = len(sdf.columns) # note - wrap this into a fuc\n",
    "    print(\"The {} dataset has shape of ({},{}) and following dtypes\\n{}\".\\\n",
    "        format(name, rc, cc, sdf.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 186:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (67542878,9) and following dtypes\n",
      "[('event_time', 'string'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_id', 'int'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for f in glob.glob(DATA_IN+\"*.gz\"):\n",
    "    if \"events\" not in locals():\n",
    "        events = ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\")\n",
    "    else:\n",
    "        events.union(ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\"))\n",
    "#events = ss.read.csv(glob.glob(DATA_DIR+\"*.gz\")[2],header=True, inferSchema=True)\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep interactions for users with >=1 trans\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "#users_to_keep = events.filter(col(\"event_type\")==\"purchase\").\\\n",
    "#    groupBy(\"user_id\").agg(countDistinct(\"user_session\").\\\n",
    "#        alias(\"n_transactions\")).filter(col(\"n_transactions\")>=1).select(col(\"user_id\"))\n",
    "#events = events.join(users_to_keep, [\"user_id\"], \"inner\")\n",
    "#get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 188:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (67542878,9) and following dtypes\n",
      "[('event_time', 'timestamp'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_id', 'int'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert dates\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "events = events.withColumn(\"event_time\", to_timestamp(col(\"event_time\")))\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The products dataset has shape of (214999,3) and following dtypes\n",
      "[('product_id', 'int'), ('category_id', 'bigint'), ('brand', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories dataset has shape of (1162,2) and following dtypes\n",
      "[('category_id', 'bigint'), ('category_code', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 196:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The events dataset has shape of (67542878,6) and following dtypes\n",
      "[('event_time', 'timestamp'), ('event_type', 'string'), ('user_id', 'int'), ('product_id', 'int'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# carve-out product-cat info as it is duplicated\n",
    "products = events.select([\"product_id\", \"category_id\", \"brand\"]).dropDuplicates()\n",
    "categories = events.select([\"category_id\", \"category_code\"]).dropDuplicates()\n",
    "events = events.select([\"event_time\", \"event_type\", \"user_id\", \"product_id\", \"price\",\"user_session\"])\n",
    "get_sdf_info(products, \"products\")\n",
    "get_sdf_info(categories, \"categories\")\n",
    "get_sdf_info(events, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# try to save both of the datasets to gz\n",
    "# NOTE: ADD HEADER YOU ANIMAL\n",
    "import os\n",
    "if not os.path.exists(DATA_OUT):\n",
    "    os.makedirs(DATA_OUT)\n",
    "events.write.csv(DATA_OUT+\"events\", compression=\"gzip\", header=True)\n",
    "products.write.csv(DATA_OUT+\"products\", compression=\"gzip\", header=True)\n",
    "categories.write.csv(DATA_OUT+\"categories\", compression=\"gzip\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the product table - both of them small enough to do this firstly in pandas and rewriting it to spark\n",
    "\n",
    "# product id\n",
    "# category id\n",
    "# collapsed brand name\n",
    "\n",
    "# category table\n",
    "\n",
    "# category id\n",
    "# category name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180907"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176133"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\").select(\"_c0\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180907"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\").select([\"_c0\",\"_c3\"]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the products differ only in the brand name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='100035628', count=2),\n",
       " Row(_c0='1307579', count=2),\n",
       " Row(_c0='12719061', count=2),\n",
       " Row(_c0='29501900', count=2),\n",
       " Row(_c0='100055424', count=2),\n",
       " Row(_c0='100052322', count=2),\n",
       " Row(_c0='100062425', count=2),\n",
       " Row(_c0='100042748', count=2),\n",
       " Row(_c0='10501054', count=2),\n",
       " Row(_c0='100017901', count=2),\n",
       " Row(_c0='100061898', count=2),\n",
       " Row(_c0='100053117', count=2),\n",
       " Row(_c0='100040716', count=2),\n",
       " Row(_c0='100029997', count=2),\n",
       " Row(_c0='100027014', count=3),\n",
       " Row(_c0='100056240', count=3),\n",
       " Row(_c0='100027991', count=3),\n",
       " Row(_c0='100040178', count=3),\n",
       " Row(_c0='100046228', count=3),\n",
       " Row(_c0='100046224', count=3)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\").groupBy(\"_c0\").count().orderBy(\"count\").tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+-------------+------+\n",
      "|      _c0|                _c1|          _c2|   _c3|\n",
      "+---------+-------------------+-------------+------+\n",
      "|100046224|2053013558978937451|sport.bicycle|  null|\n",
      "|100046224|2053013558978937451|sport.bicycle|  nike|\n",
      "|100046224|2053013558978937451|sport.bicycle|jordan|\n",
      "+---------+-------------------+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\").filter(col(\"_c0\")==100046224).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/mnt/Data/git_root/churn-modeling/data/intermediate/rees46/products",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Data/git_root/churn-modeling/jupyters/01-rees46-data-init-eda.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/Data/git_root/churn-modeling/jupyters/01-rees46-data-init-eda.ipynb#ch0000020?line=0'>1</a>\u001b[0m ss\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(DATA_OUT\u001b[39m+\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mproducts\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39m_c2\u001b[39m.\u001b[39mfactorize()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py:737\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/readwriter.py?line=734'>735</a>\u001b[0m     path \u001b[39m=\u001b[39m [path]\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/readwriter.py?line=735'>736</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/sql/readwriter.py?line=736'>737</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/readwriter.py?line=737'>738</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/readwriter.py?line=738'>739</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1297'>1298</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1298'>1299</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1299'>1300</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1300'>1301</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1306'>1307</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Path does not exist: file:/mnt/Data/git_root/churn-modeling/data/intermediate/rees46/products"
     ]
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\")._c2.factorize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[7203086: string, 2232732084546306225: string, furniture.living_room.chair: string, _c3: string]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.read.csv(DATA_OUT+\"products\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, countDistinct"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f541d2ce529b630cdc96c7c32eb4a5a175871233e2dcc03350c8ccd9d2e580"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
