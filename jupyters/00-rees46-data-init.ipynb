{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = \"../data/raw/rees46/_/\"\n",
    "DATA_OUT = \"../data/raw/rees46/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# later on - probably add dtypes in load phase\n",
    "from pyspark.sql.types import StringType, IntegerType,\\\n",
    "    DoubleType, StructType, StructField, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '10g'), ('spark.executor.cores', '4'),\n",
    "    ('spark.cores.max', '4'), ('spark.driver.memory','8g'), (\"spark.kryoserializer.buffer.max\",\"1g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")])\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "# terminate # set parames # get or create\n",
    "ss.sparkContext.setLogLevel(\"ERROR\")\n",
    "ss.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_info(sdf, name=\"\"):\n",
    "    rc = sdf.count(); cc = len(sdf.columns) # note - wrap this into a fuc\n",
    "    print(\"The {} dataset has shape of ({},{}), and following dtypes\\n{}\".\\\n",
    "        format(name, rc, cc, sdf.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for f in glob.glob(DATA_IN+\"*.gz\"):\n",
    "    if \"events\" not in locals():\n",
    "        events = ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\")\n",
    "    else:\n",
    "        events = events.union(ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\"))\n",
    "#events = ss.read.csv(glob.glob(DATA_DIR+\"*.gz\")[2],header=True, inferSchema=True)\n",
    "get_sdf_info(events)\n",
    "events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep interactions for users with >=10 trans\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "users_to_keep = events.filter(col(\"event_type\")==\"purchase\").\\\n",
    "    groupBy(\"user_id\").agg(countDistinct(\"user_session\").\\\n",
    "        alias(\"n_transactions\")).filter(col(\"n_transactions\")>=10).select(col(\"user_id\"))\n",
    "events = events.join(users_to_keep, [\"user_id\"], \"inner\")\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carry-out date conversion\n",
    "from pyspark.sql.functions import col,to_timestamp\n",
    "events = events.withColumn(\"event_time\", to_timestamp(col(\"event_time\")))\n",
    "get_sdf_info(events)\n",
    "events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "cols_old = events.columns\n",
    "cols_to_replace = [\"user_id\", \"product_id\", \"category_id\", \"event_type\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\n",
    "    for column in cols_to_replace]\n",
    "sip = Pipeline(stages=indexers)\n",
    "sip = sip.fit(events)\n",
    "events = sip.transform(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup for events\n",
    "event_types = events.select(col(\"event_type_index\").alias(\"event_type_id\"),\n",
    "    col(\"event_type\").alias(\"event_type_name\")).dropDuplicates()\n",
    "for c in cols_to_replace:\n",
    "    events = events.withColumn(c,col(c+\"_index\"))\n",
    "events = events.withColumn(\"event_type_id\", col(\"event_type\"))\n",
    "# carve-out tabs\n",
    "# NOTE: CONSIDER INDEXING SESSIONS\n",
    "products = events.select([\"product_id\", \"category_id\", \"brand\"]).dropDuplicates()\n",
    "categories = events.select([\"category_id\", \"category_code\"]).dropDuplicates()\n",
    "events = events.select([\"event_time\", \"user_id\", \"product_id\", \"event_type_id\", \"price\", \"user_session\"])\n",
    "get_sdf_info(products, \"products\")\n",
    "get_sdf_info(categories, \"categories\")\n",
    "get_sdf_info(events, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to save both of the datasets to gz\n",
    "events.write.csv(DATA_OUT+\"events\", compression=\"gzip\", mode=\"overwrite\", header=True)\n",
    "products.write.csv(DATA_OUT+\"products\", compression=\"gzip\", mode=\"overwrite\", header=True)\n",
    "categories.write.csv(DATA_OUT+\"categories\", compression=\"gzip\", mode=\"overwrite\", header=True)\n",
    "event_types.write.csv(DATA_OUT+\"event_types\", compression=\"gzip\", mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on tree-based mapping\n",
    "import pandas as pd\n",
    "cat_df = ss.read.csv(DATA_OUT+\"categories\", header=True).toPandas()\n",
    "\n",
    "# we might use this mapping\n",
    "def get_edges(leaf_id, category_code):\n",
    "    if category_code is None:\n",
    "        cat=[]\n",
    "    else:\n",
    "        cat = category_code.split(\".\")\n",
    "    res = []\n",
    "    prev = leaf_id\n",
    "    while len(cat)>0:\n",
    "        curr = cat.pop()\n",
    "        res.append([curr, prev])\n",
    "        prev=curr\n",
    "    if len(cat)==0:\n",
    "        res.append([None,prev])\n",
    "    return pd.DataFrame(res,\n",
    "        columns=[\"parent_category\", \"category\"])\n",
    "\n",
    "tree = []\n",
    "for i,r in cat_df.iterrows():\n",
    "    tree.append(get_edges(r[\"category_id\"], r[\"category_code\"]))\n",
    "pd.concat(tree)        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
