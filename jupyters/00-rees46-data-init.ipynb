{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = \"../data/raw/rees46/_/\"\n",
    "DATA_OUT = \"../data/raw/rees46/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# later on - probably add dtypes in load phase\n",
    "from pyspark.sql.types import StringType, IntegerType,\\\n",
    "    DoubleType, StructType, StructField, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/17 10:42:44 WARN Utils: Your hostname, mf-H110M-S2H resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface enx00d2b176ce73)\n",
      "22/03/17 10:42:44 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/17 10:42:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.executor.memory', '10g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.driver.port', '45283'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.app.id', 'local-1647510166789'),\n",
       " ('spark.kryoserializer.buffer.max', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.driver.host', '192.168.0.108'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'true'),\n",
       " ('spark.app.startTime', '1647510166722')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '10g'), ('spark.executor.cores', '4'),\n",
    "    ('spark.cores.max', '4'), ('spark.driver.memory','8g'), (\"spark.kryoserializer.buffer.max\",\"1g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")])\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "# terminate # set parames # get or create\n",
    "ss.sparkContext.setLogLevel(\"ERROR\")\n",
    "ss.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_info(sdf, name=\"\"):\n",
    "    rc = sdf.count(); cc = len(sdf.columns) # note - wrap this into a fuc\n",
    "    print(\"The {} dataset has shape of ({},{}), and following dtypes\\n{}\".\\\n",
    "        format(name, rc, cc, sdf.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "for f in glob.glob(DATA_IN+\"*.gz\"):\n",
    "    if \"events\" not in locals():\n",
    "        events = ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\")\n",
    "    else:\n",
    "        events.union(ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\"))\n",
    "#events = ss.read.csv(glob.glob(DATA_DIR+\"*.gz\")[2],header=True, inferSchema=True)\n",
    "get_sdf_info(events)\n",
    "events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = ss.read.csv(DATA_OUT+\"events\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:===================================================>   (189 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (1806685,6), and following dtypes\n",
      "[('user_id', 'string'), ('event_time', 'string'), ('event_type', 'string'), ('product_id', 'string'), ('price', 'string'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keep interactions for users with >=10 trans\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "users_to_keep = events.filter(col(\"event_type\")==\"purchase\").\\\n",
    "    groupBy(\"user_id\").agg(countDistinct(\"user_session\").\\\n",
    "        alias(\"n_transactions\")).filter(col(\"n_transactions\")>=10).select(col(\"user_id\"))\n",
    "events = events.join(users_to_keep, [\"user_id\"], \"inner\")\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode cats\n",
    "def encode_column(df, column_name, replace=False):\n",
    "    from pyspark.sql.functions import col, lit, create_map\n",
    "    from itertools import chain\n",
    "    vals = [c[column_name]for c in df.select(column_name).distinct().collect()]\n",
    "    vals_dict = {v:i for i,v in enumerate(vals)}\n",
    "    map_exp =  create_map([lit(x) for x in chain(*vals_dict.items())])\n",
    "    fcol = \"enc_\"\n",
    "    if replace:\n",
    "        fcol = \"\"\n",
    "    return df.withColumn(fcol+column_name,\n",
    "        map_exp.getItem(col(column_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (1806685,6), and following dtypes\n",
      "[('user_id', 'string'), ('event_time', 'timestamp'), ('event_type', 'string'), ('product_id', 'string'), ('price', 'string'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+----------+------+--------------------+\n",
      "|  user_id|         event_time|event_type|product_id| price|        user_session|\n",
      "+---------+-------------------+----------+----------+------+--------------------+\n",
      "|512389344|2019-12-02 10:39:34|      view|   1005177|849.16|8f3c8447-ed83-4c0...|\n",
      "|512389344|2019-12-02 10:39:36|      cart|   1005177|849.16|8f3c8447-ed83-4c0...|\n",
      "|512389344|2019-12-02 10:41:46|  purchase|   1005177|849.16|8f3c8447-ed83-4c0...|\n",
      "+---------+-------------------+----------+----------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# carry-out date conversion and remapping\n",
    "from pyspark.sql.functions import col,to_timestamp\n",
    "events = events.withColumn(\"event_time\", to_timestamp(col(\"event_time\")))\n",
    "#cols_to_replace = [\"user_session\", \"user_id\", \"product_id\", \"category_id\"]\n",
    "#for c in cols_to_replace:\n",
    "#    events = encode_column(events, c, replace=True)\n",
    "get_sdf_info(events)\n",
    "events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o150.fit.\n: org.apache.spark.SparkException: Input column category_id does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m/mnt/Data/git_root/churn-modeling/jupyters/00-rees46-data-init.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Data/git_root/churn-modeling/jupyters/00-rees46-data-init.ipynb#ch0000010?line=3'>4</a>\u001b[0m indexers \u001b[39m=\u001b[39m [StringIndexer(inputCol\u001b[39m=\u001b[39mcolumn, outputCol\u001b[39m=\u001b[39mcolumn\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m_index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Data/git_root/churn-modeling/jupyters/00-rees46-data-init.ipynb#ch0000010?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m column \u001b[39min\u001b[39;00m cols_to_replace]\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Data/git_root/churn-modeling/jupyters/00-rees46-data-init.ipynb#ch0000010?line=5'>6</a>\u001b[0m pipeline \u001b[39m=\u001b[39m Pipeline(stages\u001b[39m=\u001b[39mindexers)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/mnt/Data/git_root/churn-modeling/jupyters/00-rees46-data-init.ipynb#ch0000010?line=6'>7</a>\u001b[0m pipeline \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39;49mfit(events)\n\u001b[1;32m      <a href='vscode-notebook-cell:/mnt/Data/git_root/churn-modeling/jupyters/00-rees46-data-init.ipynb#ch0000010?line=7'>8</a>\u001b[0m events \u001b[39m=\u001b[39m pipeline\u001b[39m.\u001b[39mtransform(events)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/pipeline.py:114\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/pipeline.py?line=111'>112</a>\u001b[0m     dataset \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39mtransform(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/pipeline.py?line=112'>113</a>\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/ml/pipeline.py?line=113'>114</a>\u001b[0m     model \u001b[39m=\u001b[39m stage\u001b[39m.\u001b[39;49mfit(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/pipeline.py?line=114'>115</a>\u001b[0m     transformers\u001b[39m.\u001b[39mappend(model)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/pipeline.py?line=115'>116</a>\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:161\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=158'>159</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy(params)\u001b[39m.\u001b[39m_fit(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=159'>160</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/ml/base.py?line=160'>161</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=161'>162</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=162'>163</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/base.py?line=163'>164</a>\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:335\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=333'>334</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_fit\u001b[39m(\u001b[39mself\u001b[39m, dataset):\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=334'>335</a>\u001b[0m     java_model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_java(dataset)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=335'>336</a>\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=336'>337</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/wrapper.py:332\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=317'>318</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=318'>319</a>\u001b[0m \u001b[39mFits a Java model to the input dataset.\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=319'>320</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=328'>329</a>\u001b[0m \u001b[39m    fitted Java model\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=329'>330</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=330'>331</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/ml/wrapper.py?line=331'>332</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_java_obj\u001b[39m.\u001b[39;49mfit(dataset\u001b[39m.\u001b[39;49m_jdf)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1297'>1298</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1298'>1299</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1299'>1300</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1300'>1301</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1306'>1307</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=108'>109</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdeco\u001b[39m(\u001b[39m*\u001b[39ma, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw):\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=109'>110</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=110'>111</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49ma, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=111'>112</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m py4j\u001b[39m.\u001b[39mprotocol\u001b[39m.\u001b[39mPy4JJavaError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    <a href='file:///opt/spark/python/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m         converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=323'>324</a>\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=324'>325</a>\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=325'>326</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=326'>327</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=327'>328</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=328'>329</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=329'>330</a>\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=330'>331</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py?line=331'>332</a>\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o150.fit.\n: org.apache.spark.SparkException: Input column category_id does not exist.\n\tat org.apache.spark.ml.feature.StringIndexerBase.$anonfun$validateAndTransformSchema$2(StringIndexer.scala:128)\n\tat scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:245)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.flatMap(TraversableLike.scala:245)\n\tat scala.collection.TraversableLike.flatMap$(TraversableLike.scala:242)\n\tat scala.collection.mutable.ArrayOps$ofRef.flatMap(ArrayOps.scala:198)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema(StringIndexer.scala:123)\n\tat org.apache.spark.ml.feature.StringIndexerBase.validateAndTransformSchema$(StringIndexer.scala:115)\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:145)\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:252)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:71)\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:237)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "cols_to_replace = [\"user_id\", \"product_id\", \"category_id\", \"event_type\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\n",
    "    for column in cols_to_replace]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "pipeline = pipeline.fit(events)\n",
    "events = pipeline.transform(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carve-out some of the info as it is duplicated\n",
    "products = events.select([\"product_id\", \"category_id\", \"brand\"]).dropDuplicates()\n",
    "categories = events.select([\"category_id\", \"category_code\"]).dropDuplicates()\n",
    "events = events.select([\"event_time\", \"event_type\", \"user_id\", \"product_id\", \"price\", \"user_session\"])\n",
    "get_sdf_info(products, \"products\")\n",
    "get_sdf_info(categories, \"categories\")\n",
    "get_sdf_info(events, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to save both of the datasets to gz\n",
    "#import os, shutil\n",
    "#shutil.rmtree(DATA_OUT, ignore_errors=True)\n",
    "#os.makedirs(DATA_OUT)\n",
    "#events.write.csv(DATA_OUT+\"events\", compression=\"gzip\", header=True)\n",
    "#products.write.csv(DATA_OUT+\"products\", compression=\"gzip\", header=True)\n",
    "#categories.write.csv(DATA_OUT+\"categories\", compression=\"gzip\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "si = StringIndexer(inputCol=\"category_id\", outputCol=\"category_id_encoded\")\n",
    "si = si.fit(categories)\n",
    "si.transform(categories).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = ss.read.csv(DATA_OUT+\"products\", header=True)\n",
    "products.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si.transform(products).filter(col(\"category_id\")==2053013552259662037).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the product table - both of them small enough to do this firstly in pandas and rewriting it to spark\n",
    "\n",
    "# product id\n",
    "# category id\n",
    "# collapsed brand name\n",
    "\n",
    "# category table\n",
    "\n",
    "# category id\n",
    "# category name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on tree-based mapping\n",
    "import pandas as pd\n",
    "cat_df = ss.read.csv(DATA_OUT+\"categories\", header=True).toPandas()\n",
    "\n",
    "# we might use this mapping\n",
    "def get_edges(leaf_id, category_code):\n",
    "    if category_code is None:\n",
    "        cat=[]\n",
    "    else:\n",
    "        cat = category_code.split(\".\")\n",
    "    res = []\n",
    "    prev = leaf_id\n",
    "    while len(cat)>0:\n",
    "        curr = cat.pop()\n",
    "        res.append([curr, prev])\n",
    "        prev=curr\n",
    "    if len(cat)==0:\n",
    "        res.append([None,prev])\n",
    "    return pd.DataFrame(res,\n",
    "        columns=[\"parent_category\", \"category\"])\n",
    "\n",
    "tree = []\n",
    "for i,r in cat_df.iterrows():\n",
    "    tree.append(get_edges(r[\"category_id\"], r[\"category_code\"]))\n",
    "pd.concat(tree)        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f541d2ce529b630cdc96c7c32eb4a5a175871233e2dcc03350c8ccd9d2e580"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
