{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_IN = \"../data/raw/rees46/_/\"\n",
    "DATA_OUT = \"../data/raw/rees46/filtered/\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark \n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# later on - probably add dtypes in load phase\n",
    "from pyspark.sql.types import StringType, IntegerType,\\\n",
    "    DoubleType, StructType, StructField, DateType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/19 11:27:39 WARN Utils: Your hostname, mf-H110M-S2H resolves to a loopback address: 127.0.1.1; using 192.168.0.108 instead (on interface enx00d2b176ce73)\n",
      "22/04/19 11:27:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/04/19 11:27:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('spark.driver.port', '33653'),\n",
       " ('spark.app.startTime', '1650360462281'),\n",
       " ('spark.executor.memory', '10g'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.app.name', 'pyspark-shell'),\n",
       " ('spark.executor.cores', '4'),\n",
       " ('spark.cores.max', '4'),\n",
       " ('spark.kryoserializer.buffer.max', '1g'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.driver.memory', '8g'),\n",
       " ('spark.driver.host', '192.168.0.108'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.master', 'local[*]'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.app.id', 'local-1650360462342'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.execution.arrow.pyspark.enabled', 'true')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "conf = pyspark.SparkConf().setAll([('spark.executor.memory', '10g'), ('spark.executor.cores', '4'),\n",
    "    ('spark.cores.max', '4'), ('spark.driver.memory','8g'), (\"spark.kryoserializer.buffer.max\",\"1g\"),\n",
    "    (\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")])\n",
    "sc.stop()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "# terminate # set parames # get or create\n",
    "ss.sparkContext.setLogLevel(\"ERROR\")\n",
    "ss.sparkContext.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sdf_info(sdf, name=\"\"):\n",
    "    rc = sdf.count(); cc = len(sdf.columns) # note - wrap this into a fuc\n",
    "    print(\"The {} dataset has shape of ({},{}), and following dtypes\\n{}\".\\\n",
    "        format(name, rc, cc, sdf.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (67542878,9), and following dtypes\n",
      "[('event_time', 'string'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_id', 'int'), ('user_session', 'string')]\n",
      "+--------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+\n",
      "|          event_time|event_type|product_id|        category_id|       category_code|brand|  price|  user_id|        user_session|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+\n",
      "|2019-12-01 00:00:...|      view|   1005105|2232732093077520756|construction.tool...|apple|1302.48|556695836|ca5eefc5-11f9-450...|\n",
      "|2019-12-01 00:00:...|      view|  22700068|2232732091643068746|                null|force| 102.96|577702456|de33debe-c7bf-44e...|\n",
      "|2019-12-01 00:00:...|      view|   2402273|2232732100769874463|appliances.person...|bosch| 313.52|539453785|5ee185a7-0689-4a3...|\n",
      "+--------------------+----------+----------+-------------------+--------------------+-----+-------+---------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "for f in glob.glob(DATA_IN+\"*.gz\"):\n",
    "    if \"events\" not in locals():\n",
    "        events = ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\")\n",
    "    else:\n",
    "        events.union(ss.read.csv(f,header=True, inferSchema=True, nanValue=\"null\"))\n",
    "#events = ss.read.csv(glob.glob(DATA_DIR+\"*.gz\")[2],header=True, inferSchema=True)\n",
    "get_sdf_info(events)\n",
    "events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:=================================================>    (182 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (1806685,9), and following dtypes\n",
      "[('user_id', 'int'), ('event_time', 'string'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# keep interactions for users with >=10 trans\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "users_to_keep = events.filter(col(\"event_type\")==\"purchase\").\\\n",
    "    groupBy(\"user_id\").agg(countDistinct(\"user_session\").\\\n",
    "        alias(\"n_transactions\")).filter(col(\"n_transactions\")>=10).select(col(\"user_id\"))\n",
    "events = events.join(users_to_keep, [\"user_id\"], \"inner\")\n",
    "get_sdf_info(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  dataset has shape of (1806685,9), and following dtypes\n",
      "[('user_id', 'int'), ('event_time', 'timestamp'), ('event_type', 'string'), ('product_id', 'int'), ('category_id', 'bigint'), ('category_code', 'string'), ('brand', 'string'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+\n",
      "|  user_id|         event_time|event_type|product_id|        category_id|       category_code|  brand| price|        user_session|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+\n",
      "|512415830|2019-12-17 11:43:24|      view|  26300742|2053013554725912943|appliances.kitche...|lucente|320.73|70641837-0ad0-41c...|\n",
      "|512415830|2019-12-17 11:43:54|      cart|  26300742|2053013554725912943|appliances.kitche...|lucente|320.73|70641837-0ad0-41c...|\n",
      "|512415830|2019-12-17 11:44:10|  purchase|  26300742|2053013554725912943|appliances.kitche...|lucente|320.73|70641837-0ad0-41c...|\n",
      "+---------+-------------------+----------+----------+-------------------+--------------------+-------+------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# carry-out date conversion\n",
    "from pyspark.sql.functions import col,to_timestamp\n",
    "events = events.withColumn(\"event_time\", to_timestamp(col(\"event_time\")))\n",
    "get_sdf_info(events)\n",
    "events.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "cols_old = events.columns\n",
    "cols_to_replace = [\"user_id\", \"product_id\", \"category_id\", \"event_type\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\")\n",
    "    for column in cols_to_replace]\n",
    "sip = Pipeline(stages=indexers)\n",
    "sip = sip.fit(events)\n",
    "events = sip.transform(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The products dataset has shape of (81586,3), and following dtypes\n",
      "[('product_id', 'double'), ('category_id', 'double'), ('brand', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The categories dataset has shape of (1065,2), and following dtypes\n",
      "[('category_id', 'double'), ('category_code', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:==================================================>   (187 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The events dataset has shape of (1806685,6), and following dtypes\n",
      "[('event_time', 'timestamp'), ('user_id', 'double'), ('product_id', 'double'), ('event_type_id', 'double'), ('price', 'double'), ('user_session', 'string')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# lookup for events\n",
    "event_types = events.select(col(\"event_type_index\").alias(\"event_type_id\"),\n",
    "    col(\"event_type\").alias(\"event_type_name\")).dropDuplicates()\n",
    "for c in cols_to_replace:\n",
    "    events = events.withColumn(c,col(c+\"_index\"))\n",
    "events = events.withColumn(\"event_type_id\", col(\"event_type\"))\n",
    "# carve-out tabs\n",
    "products = events.select([\"product_id\", \"category_id\", \"brand\"]).dropDuplicates()\n",
    "categories = events.select([\"category_id\", \"category_code\"]).dropDuplicates()\n",
    "events = events.select([\"event_time\", \"user_id\", \"product_id\", \"event_type_id\", \"price\", \"user_session\"])\n",
    "get_sdf_info(products, \"products\")\n",
    "get_sdf_info(categories, \"categories\")\n",
    "get_sdf_info(events, \"events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# try to save both of the datasets to gz\n",
    "import os, shutil\n",
    "shutil.rmtree(DATA_OUT, ignore_errors=True)\n",
    "os.makedirs(DATA_OUT)\n",
    "events.write.csv(DATA_OUT+\"events\", compression=\"gzip\", header=True)\n",
    "products.write.csv(DATA_OUT+\"products\", compression=\"gzip\", header=True)\n",
    "categories.write.csv(DATA_OUT+\"categories\", compression=\"gzip\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on tree-based mapping\n",
    "import pandas as pd\n",
    "cat_df = ss.read.csv(DATA_OUT+\"categories\", header=True).toPandas()\n",
    "\n",
    "# we might use this mapping\n",
    "def get_edges(leaf_id, category_code):\n",
    "    if category_code is None:\n",
    "        cat=[]\n",
    "    else:\n",
    "        cat = category_code.split(\".\")\n",
    "    res = []\n",
    "    prev = leaf_id\n",
    "    while len(cat)>0:\n",
    "        curr = cat.pop()\n",
    "        res.append([curr, prev])\n",
    "        prev=curr\n",
    "    if len(cat)==0:\n",
    "        res.append([None,prev])\n",
    "    return pd.DataFrame(res,\n",
    "        columns=[\"parent_category\", \"category\"])\n",
    "\n",
    "tree = []\n",
    "for i,r in cat_df.iterrows():\n",
    "    tree.append(get_edges(r[\"category_id\"], r[\"category_code\"]))\n",
    "pd.concat(tree)        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "78f541d2ce529b630cdc96c7c32eb4a5a175871233e2dcc03350c8ccd9d2e580"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('py-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
