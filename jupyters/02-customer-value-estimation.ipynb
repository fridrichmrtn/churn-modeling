{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer value estimation\n",
    "\n",
    "\n",
    "#### Product margin\n",
    "\n",
    "Unfortunately, margin on user-item transaction are omitted in the available datasets. To include the profit aspect of the customer relationship management, we propose simulation approach describing changes in product margin over time, which are consequently used for estimating customer profit. That is, for each of the product, we draw from initial random normal distribution. This draw serves as a starting point for the random walk, which we simulate using draws from random normal distribution for step difference and put them together using cumulative sum. In other words, we use one dimensional random walk with random normal steps. For product *p* in time *t*, we estimate the margin *m* like this:\n",
    "   \n",
    "$$m^{p}_t = Normal(\\mu_0, \\sigma_0)+\\sum^{t}_{n=1}{Normal(\\mu_{diff}, \\sigma_{diff})}$$\n",
    "\n",
    "where the first element represents the starting draw, and the second element represents the cumulative sum of difference draws. For simplicity, we assume that initial variability across products and variability of the product, within the observed period, are the same. Thus, we set $\\mu_{diff} = 0$, and $\\sigma_{diff}=\\frac{\\sigma_0}{\\sqrt{t}}$.  As a result, we are able to estimate product profit with respect to just parameters of the initial random draw $\\mu_{0}$ and $\\sigma_{0}$.\n",
    "\n",
    "#### Customer value\n",
    "\n",
    "The customer value is computed as cumulative average profit using product revenue and simulated margin, result is scaled to reflect the target window size. This approach allows us to differentiate customer profit on individual level, indicate changes in customer behavior over time, and is useful in time-framed campaigns.\n",
    "Cumulative average scales down value of customers with prolonged periods without a transaction, this aspect may be taken further using windowing or explicit decay function.\n",
    "On the other hand, it is fairly limited by the window size and does not reflect on lifecycle length such as $CLV$. Let as have a customer $i$ at time $t$, the customer value can be computed as $$ CV^{i}_t = \\frac{n_t}{t}\\sum^{t}_{n=1}{m^{p}_n  r^{p}_n} $$\n",
    "\n",
    "where $n_t$ represent the length of the target window, $m^{p}_n$ stands for simulated margin for product $p$ in the time $n$, and $r^{p}_n$ is the revenue from transaction realized on product $p$ in the time $n$.\n",
    "Overall customer profit can be simply reconstructed with knowledge of the relationship length, same is true for CLV.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from delta import *\n",
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql import SparkSession\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt    \n",
    "from multiprocessing import Pool  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "def simulate_margin(products, dates, loc, scale):\n",
    "    n_products = len(products)\n",
    "    n_days = len(dates)    \n",
    "\n",
    "    scale_diff = scale/np.sqrt(n_days)\n",
    "    product_baseline = np.random.normal(loc=loc,scale=scale,size=(n_products,1))\n",
    "    product_diff = np.random.normal(loc=0, scale=scale_diff, size=(n_products, n_days-1))\n",
    "    margins = np.cumsum(np.concatenate([product_baseline, product_diff], axis=1), axis=1)\n",
    "    margins = pd.DataFrame(margins, columns=dates)\n",
    "    margins[\"product_id\"] = products\n",
    "    return margins\n",
    "\n",
    "def plot_simulated_data(data, setup):\n",
    "    x, y, hue = setup[\"x\"], setup[\"y\"], setup[\"hue\"]\n",
    "    labs = setup[\"labs\"]\n",
    "    fig, axs = plt.subplots(1,2, figsize=(20,7.5))\n",
    "    # walk\n",
    "    sns.lineplot(data=data,\n",
    "        x=x, y=y, hue=hue,\n",
    "        legend=False, palette=sns.color_palette(\"rocket_r\", as_cmap=True),\n",
    "        alpha=0.5, ax=axs[0]);\n",
    "    axs[0].set_ylabel(labs[\"y\"]);\n",
    "    axs[0].tick_params(axis='x', rotation=90);\n",
    "    axs[0].set_xlabel(labs[\"x\"]);\n",
    "    # hist\n",
    "    sns.histplot(data=data,\n",
    "        x=y, bins=50,\n",
    "        color=sns.color_palette(\"rocket\",10)[1],\n",
    "        ax=axs[1]);\n",
    "    axs[1].set_ylabel(\"frequency\");\n",
    "    axs[1].set_xlabel(labs[\"y\"]);\n",
    "\n",
    "def get_purchases(path=\"../data/delta/rees46/events\"):\n",
    "    builder = SparkSession.builder.appName(\"reader\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "    events = spark.read.format(\"delta\").load(path)\n",
    "    purchases = events.where(f.col(\"event_type_name\")==\"purchase\")\\\n",
    "        .select(\"product_id\",\"user_id\",\"revenue\",\"event_time\")\\\n",
    "            .toPandas()\n",
    "    spark.stop()\n",
    "    purchases = _optimize_numeric_dtypes(purchases)\n",
    "    purchases.loc[:,\"date\"] = purchases.event_time.dt.date\n",
    "    purchases[\"week_start\"] = purchases[\"date\"] -\\\n",
    "        pd.to_timedelta(purchases[\"event_time\"].dt.dayofweek, unit=\"d\")\n",
    "    return purchases.loc[:,[\"product_id\",\"user_id\",\"revenue\",\"date\",\"week_start\"]]    \n",
    "\n",
    "def get_cust_value(purchases, margins, target_size=4):\n",
    "    purchases = purchases.merge(margins, on=[\"product_id\",\"date\"], how=\"inner\")\n",
    "    purchases[\"profit\"] = purchases.revenue*purchases.margin\n",
    "    purchases = purchases.groupby([\"user_id\", \"week_start\"], as_index=False)\\\n",
    "        .agg(profit=(\"profit\", sum), revenue=(\"revenue\", sum))\n",
    "    user_week_product = product(purchases.user_id.unique(), purchases.week_start.unique())\n",
    "    user_cap = pd.DataFrame(user_week_product, columns=[\"user_id\", \"week_start\"])\\\n",
    "        .merge(purchases, on=[\"user_id\", \"week_start\"], how=\"left\")\\\n",
    "            .fillna(0).sort_values(\"week_start\")\\\n",
    "                .rename(columns={\"week_start\":\"date\"})\n",
    "    user_cap[\"cv\"] = target_size*user_cap.groupby(\"user_id\").profit.cumsum()/\\\n",
    "        (user_cap.groupby(\"user_id\").cumcount()+1)\n",
    "    return user_cap.loc[:,[\"user_id\", \"date\", \"cv\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simulated product margins\n",
    "\n",
    "In the plots below, we show simulated margins for the `rees46` datasets. On the left, we see margin changes over time - the random walk. On the right we see overall margin distribution. Both plots are in line with the parameters of the simulation and reflects common e-commerce retail business - most of the products show positive margin, however there are a few products generating loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATE MARGINS\n",
    "purchases = get_purchases()\n",
    "# simulation\n",
    "loc = 0.15\n",
    "scale = 0.05\n",
    "dates = purchases.sort_values(\"week_start\").week_start.unique()\n",
    "products = purchases.product_id.unique()\n",
    "margins = simulate_margin(products, dates, loc, scale)\n",
    "margins = pd.melt(margins, id_vars=[\"product_id\"],\n",
    "        var_name=\"date\", value_name=\"margin\")\n",
    "# plot\n",
    "prod_subset = margins.product_id.isin(margins.product_id.unique()[:1000])\n",
    "plot_conf = {\"x\":\"date\", \"y\":\"margin\", \"hue\":\"product_id\", \"labs\":{\"x\":\"date\", \"y\":\"$m^p_t$\"}}    \n",
    "plot_simulated_data(margins[prod_subset], plot_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated CV\n",
    "\n",
    "Now, with estimated margins one can take a look at development of customer's cumulative average profit and its overall distribution. On the left, we see changes in $CV$ over time. The most dominant pattern consists of a peak (one or more transactions) and decay (no further transactions). Only a few customers do repeated purchases, which once again stress the importance of customer retention.\n",
    "On the right we see overal $CV$ distribution, centered around zero with right-tail. The shape is result of the said dominant pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_profit =  get_cust_value(purchases, margins, target_size=4)\n",
    "plot_conf = {\"x\":\"date\", \"y\":\"cv\", \"hue\":\"user_id\", \"labs\":{\"x\":\"date\", \"y\":\"$CV^i_t$\"}} \n",
    "plot_simulated_data(cust_profit, plot_conf)\n",
    "stats.describe(cust_profit.cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sensitivity analysis\n",
    "\n",
    "In order to examine how the simulation parameters relate to $CV$, we construct simple simulation procedure which examines changes in $E(CV)$, and its confidence bounds, with respect to simulation params $\\mu_{0}$ and $\\sigma_{0}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 21:44:09 WARN Utils: Your hostname, DESKTOP-9EJISJ5 resolves to a loopback address: 127.0.1.1; using 172.23.48.129 instead (on interface eth0)\n",
      "22/07/31 21:44:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/mf/github/churn-modeling/.env/lib/python3.8/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/mf/.ivy2/cache\n",
      "The jars for the packages stored in: /home/mf/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-11549029-3ddf-49d7-ae32-91feb915a01d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.0.0 in central\n",
      "\tfound io.delta#delta-storage;2.0.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.8 in central\n",
      "\tfound org.codehaus.jackson#jackson-core-asl;1.9.13 in central\n",
      ":: resolution report :: resolve 119ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tio.delta#delta-core_2.12;2.0.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.0.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.8 from central in [default]\n",
      "\torg.codehaus.jackson#jackson-core-asl;1.9.13 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   0   ||   4   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-11549029-3ddf-49d7-ae32-91feb915a01d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 4 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/31 21:44:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "/home/mf/github/churn-modeling/.env/lib/python3.8/site-packages/pandas/core/arrays/datetimelike.py:1189: PerformanceWarning: Adding/subtracting object-dtype array to TimedeltaArray not vectorized.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "purchases = get_purchases()\n",
    "\n",
    "def simulate_cust_value(purchases, loc, scale, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    dates = purchases.sort_values(\"week_start\").week_start.unique()\n",
    "    products = purchases.product_id.unique()\n",
    "    margins = simulate_margin(products, dates, loc, scale)\n",
    "    margins = pd.melt(margins, id_vars=[\"product_id\"],\n",
    "            var_name=\"date\", value_name=\"margin\")\n",
    "    cust_profit = get_cust_value(purchases, margins, 4)\n",
    "    # NOTE: one can get full distribution just by omitting the next step\n",
    "    cust_profit = pd.DataFrame([[loc, scale, seed, cust_profit.cv.mean(), cust_profit.cv.std()]],\n",
    "        columns=[\"loc\",\"scale\", \"seed\", \"cv\", \"sigma\"])        \n",
    "    return cust_profit\n",
    "\n",
    "def para_scv(params):\n",
    "    return simulate_cust_value(purchases,\n",
    "        params[0], params[1], params[2])\n",
    "\n",
    "def get_sim_stats(df):\n",
    "    loc = df[\"loc\"].unique()[0]\n",
    "    scale = df[\"scale\"].unique()[0]\n",
    "    mu =  df[\"cv\"].mean()\n",
    "    lb, ub =  np.percentile(df[\"cv\"], q=[2.5, 97.5])\n",
    "    ads = stats.describe(df[\"cv\"])\n",
    "    return pd.DataFrame([[loc, scale, mu, lb, ub, ads]], columns=[\"loc\",\"scale\",\"mu\",\"lb\",\"ub\",\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From print-out below, we can see that the changes in $E(CV)$ are mostly driven by $\\mu_{0}$, there are some changes in variance due to $\\sigma_{0}$, but they does not seem significant. In other words, observed $CV$ is related to the retailer's overall margin level/product portfolio health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>loc</th>\n",
       "      <th>scale</th>\n",
       "      <th>mu</th>\n",
       "      <th>lb</th>\n",
       "      <th>ub</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>10.875458</td>\n",
       "      <td>8.298016</td>\n",
       "      <td>13.681438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.10</td>\n",
       "      <td>10.916913</td>\n",
       "      <td>5.762028</td>\n",
       "      <td>16.528873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <th>0</th>\n",
       "      <td>0.05</td>\n",
       "      <td>0.15</td>\n",
       "      <td>10.958368</td>\n",
       "      <td>3.226041</td>\n",
       "      <td>19.376307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <th>0</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.05</td>\n",
       "      <td>32.543465</td>\n",
       "      <td>29.966023</td>\n",
       "      <td>35.349445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <th>0</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.10</td>\n",
       "      <td>32.584920</td>\n",
       "      <td>27.430035</td>\n",
       "      <td>38.196879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <th>0</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>32.626375</td>\n",
       "      <td>24.894048</td>\n",
       "      <td>41.044314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.05</td>\n",
       "      <td>54.211472</td>\n",
       "      <td>51.634030</td>\n",
       "      <td>57.017452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.10</td>\n",
       "      <td>54.252927</td>\n",
       "      <td>49.098042</td>\n",
       "      <td>59.864886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>0.15</td>\n",
       "      <td>54.294382</td>\n",
       "      <td>46.562055</td>\n",
       "      <td>62.712321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      loc  scale         mu         lb         ub\n",
       "0 0  0.05   0.05  10.875458   8.298016  13.681438\n",
       "1 0  0.05   0.10  10.916913   5.762028  16.528873\n",
       "2 0  0.05   0.15  10.958368   3.226041  19.376307\n",
       "3 0  0.15   0.05  32.543465  29.966023  35.349445\n",
       "4 0  0.15   0.10  32.584920  27.430035  38.196879\n",
       "5 0  0.15   0.15  32.626375  24.894048  41.044314\n",
       "6 0  0.25   0.05  54.211472  51.634030  57.017452\n",
       "7 0  0.25   0.10  54.252927  49.098042  59.864886\n",
       "8 0  0.25   0.15  54.294382  46.562055  62.712321"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# simulate\n",
    "mu_range = [0.05,0.15,0.25]\n",
    "sigma_range = [0.05, 0.1, 0.15]\n",
    "seed_range = np.random.randint(low=0, high=2**16, size=1000) # sim no\n",
    "\n",
    "with Pool(6) as poo:\n",
    "    sensitivity_simulation = poo.map(para_scv,\n",
    "        product(mu_range, sigma_range, seed_range))\n",
    "sensitivity_simulation =  pd.concat(sensitivity_simulation) \n",
    "sensitivity_summary = sensitivity_simulation.groupby(\n",
    "    [\"loc\",\"scale\"], as_index=False).apply(get_sim_stats)\n",
    "sensitivity_summary.loc[:,[\"loc\",\"scale\",\"mu\", \"lb\", \"ub\"]]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
