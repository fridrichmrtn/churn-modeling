{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### COMBINED NEURAL NETWORK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "\n",
    "class MultiOutputTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "\n",
    "        self.class_encoder_ = OneHotEncoder(sparse=False)\n",
    "        self.reg_transformer_ = PowerTransformer()\n",
    "        # Fit them to the input data\n",
    "        self.class_encoder_.fit(y_class)\n",
    "        self.reg_transformer_.fit(y_reg)\n",
    "        # Save the number of classes\n",
    "        self.n_classes_ = len(self.class_encoder_.categories_)\n",
    "        self.n_outputs_expected_ = 2\n",
    "        return self\n",
    "\n",
    "    def transform(self, y: np.ndarray) -> List[np.ndarray]:\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "        # Apply transformers to input array\n",
    "        y_class = self.class_encoder_.transform(y_class)\n",
    "        y_reg = self.reg_transformer_.transform(y_reg)\n",
    "        # Split the data into a list\n",
    "        return [y_class, y_reg]\n",
    "\n",
    "    def inverse_transform(self, y: List[np.ndarray], return_proba: bool = False) -> np.ndarray:\n",
    "        y_pred_reg = y[1]\n",
    "        if return_proba:\n",
    "            return y[0]\n",
    "        else:\n",
    "            y_pred_class = np.zeros_like(y[0])\n",
    "            y_pred_class[np.arange(len(y[0])), np.argmax(y[0], axis=1)] = 1\n",
    "            y_pred_class = self.class_encoder_.inverse_transform(y_pred_class)\n",
    "        y_pred_reg = self.reg_transformer_.inverse_transform(y_pred_reg)\n",
    "        return np.column_stack([y_pred_class, y_pred_reg])\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return {\n",
    "            \"n_classes_\": self.n_classes_,\n",
    "            \"n_outputs_expected_\": self.n_outputs_expected_,\n",
    "        }\n",
    "\n",
    "from scikeras.wrappers import BaseWrapper\n",
    "from tensorflow.keras.initializers import HeNormal, LecunNormal, HeNormal\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, concatenate, LeakyReLU\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class CombiNet(BaseWrapper):\n",
    "\n",
    "    def __init__(self, activation = \"selu\",\n",
    "        se_layers=1, se_units=256,\n",
    "        re_layers=5, re_units=100,\n",
    "        ce_layers=5, ce_units=100, cc_units=75,\n",
    "        epochs=10, verbose=0,\n",
    "        optimizer=\"adam\", optimizer__clipvalue=1.0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.activation = activation\n",
    "            self.se_layers = se_layers\n",
    "            self.se_units = se_units\n",
    "            self.re_layers = re_layers\n",
    "            self.re_units = re_units\n",
    "            self.ce_layers = ce_layers\n",
    "            self.ce_units = ce_units\n",
    "            self.cc_units = cc_units\n",
    "            self.epochs = epochs\n",
    "            self.verbose = verbose\n",
    "            self.prediction_scope = {\"classification\":0,\"regression\":1,\"full\":range(2)}\n",
    "\n",
    "    def _get_weight_init(self):\n",
    "        if isinstance(self.activation, LeakyReLU):\n",
    "            \n",
    "            init = HeNormal()\n",
    "        elif self.activation in [\"selu\", \"elu\"]:\n",
    "            init = LecunNormal()\n",
    "        else:\n",
    "            init = HeNormal()  \n",
    "        return init\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs):\n",
    "        weight_init = self._get_weight_init()\n",
    "\n",
    "        # shared extraction\n",
    "        inp = Input(shape=(self.n_features_in_))\n",
    "        fe = inp\n",
    "        for i in range(self.se_layers):\n",
    "            fe = Dense(self.se_units, self.activation,\n",
    "                kernel_initializer=weight_init)(fe)\n",
    "            fe = BatchNormalization()(fe)\n",
    "        # regression branch\n",
    "        re = fe\n",
    "        for i in range(self.re_layers):\n",
    "            re = Dense(self.re_units, self.activation,\n",
    "                kernel_initializer=weight_init)(re)\n",
    "            re = BatchNormalization()(re)\n",
    "        rr_head = Dense(1,\"linear\")(re)\n",
    "        # classification branch\n",
    "        ce = fe\n",
    "        for i in range(self.ce_layers):\n",
    "            ce = Dense(self.ce_units, self.activation,\n",
    "                kernel_initializer=weight_init)(ce)\n",
    "            ce = BatchNormalization()(ce)\n",
    "        cc = Dense(self.cc_units, self.activation,\n",
    "            kernel_initializer=weight_init)(concatenate([ce, re]))\n",
    "        cc = BatchNormalization()(cc)\n",
    "        cc_head = Dense(2, \"softmax\")(cc)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=[cc_head, rr_head])\n",
    "        model.compile(loss=[\"categorical_crossentropy\",\"mse\"], loss_weights=[.5,.5],\n",
    "            optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model\n",
    "        \n",
    "    @property\n",
    "    def target_encoder(self):\n",
    "        return MultiOutputTransformer()\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        return self.target_encoder_.inverse_transform(y_pred, return_proba=True)\n",
    "\n",
    "    def predict(self, X, scope=\"classification\"):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        y_pred = self.target_encoder_.inverse_transform(y_pred)\n",
    "        return y_pred[:,self.prediction_scope[scope]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def reduce_y(func):\n",
    "    def actual_reduce(self, X, y,  *args, **kwargs):\n",
    "        if len(y.shape)>1:\n",
    "            self.y_ = copy.copy(y)\n",
    "            y = y[:,0]\n",
    "        return func(self, X, y, *args, **kwargs)\n",
    "    return actual_reduce\n",
    "\n",
    "def expand_y(func):\n",
    "    def actual_expand(self, X, y, *args, **kwargs):\n",
    "        if len(self.y_.shape)>1:\n",
    "            y = copy.copy(self.y_)\n",
    "        return func(self, X, y, *args, **kwargs)\n",
    "    return actual_expand    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### FEATURE SELECTION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class DataFrameTransposer(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.copy().T\n",
    "\n",
    "class HierarchicalFeatureSelector(SelectorMixin, BaseEstimator):\n",
    "  \n",
    "    def __init__(self, n_features=10, alpha=0.001):\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def _get_cluster_assignments(self, data):\n",
    "        data = data.loc[:,self.results_.feature.values]\n",
    "        n_components = data.shape[1]\n",
    "        pipe = Pipeline([(\"rotate\", DataFrameTransposer()),\n",
    "            (\"pca\", GaussianRandomProjection(n_components=n_components)),\n",
    "            (\"cluster\", AgglomerativeClustering(n_clusters=self.n_features))])\n",
    "        return pipe.fit_predict(data)\n",
    "    \n",
    "    @reduce_y\n",
    "    def _get_correlations(self, X, y):\n",
    "        tf_corr = [pearsonr(y, X[c]) for c in X.columns]\n",
    "        correlations = pd.DataFrame(tf_corr, index=X.columns).reset_index()\n",
    "        correlations.columns = [\"feature\", \"r\", \"p\"]\n",
    "        correlations[\"abs_r\"] = correlations.r.abs()\n",
    "        correlations[\"sf\"] = correlations.p<=self.alpha/X.shape[1]\n",
    "        return correlations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #if len(y.shape)>1:\n",
    "        #    y = y[:,0]\n",
    "        X = pd.DataFrame(X)\n",
    "        #y = pd.Series(y)\n",
    "\n",
    "        self.in_features_ =  X.columns\n",
    "        self.results_ = self._get_correlations(X, y)\n",
    "\n",
    "        if np.sum(self.results_.sf)<= self.n_features:\n",
    "            self.best_ = self.results_[self.results_.sf]\n",
    "        else:\n",
    "            self.results_[\"cluster\"] = self._get_cluster_assignments(X)\n",
    "            self.best_ = self.results_[self.results_.sf]\\\n",
    "                .merge(self.results_.groupby(\"cluster\",\n",
    "                    as_index=False).abs_r.max(), on=[\"cluster\", \"abs_r\"])\\\n",
    "                        .drop_duplicates([\"cluster\", \"abs_r\"]).dropna()\n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        return np.array([c in set(self.best_.feature) for c in self.in_features_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### SAMPLING STRATEGIES\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "class _RandomUnderSampler(RandomUnderSampler):\n",
    "    def __init__(self, sampling_strategy=\"auto\"):\n",
    "        super().__init__(sampling_strategy)\n",
    "    @reduce_y\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        super().fit(X, y, **kwargs)\n",
    "        return self\n",
    "    @reduce_y\n",
    "    def fit_resample(self, X, y, **kwargs):\n",
    "        _ = super().fit_resample(X, y, **kwargs)\n",
    "        ind = self.sample_indices_\n",
    "        return X[ind,:], self.y_[ind]\n",
    "\n",
    "class _RandomOverSampler(RandomOverSampler):\n",
    "    def __init__(self, sampling_strategy=\"auto\"):\n",
    "        super().__init__(sampling_strategy)\n",
    "    @reduce_y\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        super().fit(X, y, **kwargs)\n",
    "        return self\n",
    "    @reduce_y\n",
    "    def fit_resample(self, X, y, **kwargs):\n",
    "        _ = super().fit_resample(X, y, **kwargs)\n",
    "        ind = self.sample_indices_\n",
    "        return X[ind,:], self.y_[ind]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# dtype opt\n",
    "def _optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "# data load\n",
    "data = _optimize_numeric_dtypes(\n",
    "    pd.read_parquet(\"../data/customer_model/retailrocket/\"))\n",
    "train = data[data.week_step>2]\n",
    "test = data[data.week_step==2]\n",
    "\n",
    "out_cols = [\"user_id\", \"target_event\", \"target_revenue\", \"week_step\",\n",
    "    \"target_cap\"]\n",
    "feat_cols = [c for c in train.columns if c not in set(out_cols)]\n",
    "target_cols = [\"target_event\", \"target_cap\"]\n",
    "\n",
    "X = train.loc[:,feat_cols].values\n",
    "y = train.loc[:,target_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mf/github/churn-modeling/.env/lib/python3.8/site-packages/sklearn/preprocessing/_data.py:3211: RuntimeWarning: overflow encountered in power\n",
      "  out[pos] = (np.power(x[pos] + 1, lmbda) - 1) / lmbda\n",
      "/home/mf/github/churn-modeling/.env/lib/python3.8/site-packages/numpy/core/_methods.py:236: RuntimeWarning: overflow encountered in multiply\n",
      "  x = um.multiply(x, x, out=x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "230/230 [==============================] - 7s 11ms/step - loss: 0.7364 - dense_124_loss: 0.6556 - dense_112_loss: 0.8172\n",
      "Epoch 2/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.5875 - dense_124_loss: 0.5798 - dense_112_loss: 0.5952\n",
      "Epoch 3/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.5397 - dense_124_loss: 0.5482 - dense_112_loss: 0.5312\n",
      "Epoch 4/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.5073 - dense_124_loss: 0.5233 - dense_112_loss: 0.4914\n",
      "Epoch 5/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.4883 - dense_124_loss: 0.5002 - dense_112_loss: 0.4763\n",
      "Epoch 6/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.4601 - dense_124_loss: 0.4663 - dense_112_loss: 0.4538\n",
      "Epoch 7/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.4236 - dense_124_loss: 0.4189 - dense_112_loss: 0.4283\n",
      "Epoch 8/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.4089 - dense_124_loss: 0.4166 - dense_112_loss: 0.4011\n",
      "Epoch 9/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.3928 - dense_124_loss: 0.3947 - dense_112_loss: 0.3910\n",
      "Epoch 10/10\n",
      "230/230 [==============================] - 2s 9ms/step - loss: 0.3664 - dense_124_loss: 0.3637 - dense_112_loss: 0.3692\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;variance_filter&#x27;, VarianceThreshold(threshold=10)),\n",
       "                (&#x27;data_scaler&#x27;, PowerTransformer()),\n",
       "                (&#x27;feature_selector&#x27;,\n",
       "                 HierarchicalFeatureSelector(n_features=50)),\n",
       "                (&#x27;data_sampler&#x27;, _RandomOverSampler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 CombiNet(batch_size=16, build_fn=None, callbacks=None, ce_layers=10, loss=None, metrics=None, model=None, optimizer=&#x27;rmsprop&#x27;, prediction_scope={&#x27;classification&#x27;: 0, &#x27;full&#x27;: range(0, 2), &#x27;regression&#x27;: 1}, random_state=None, re_layers=10, run_eagerly=False, se_layers=2, se_units=512, shuffle=True, validation_batch_size=None, validation_split=0.0, verbose=1, warm_start=False))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;variance_filter&#x27;, VarianceThreshold(threshold=10)),\n",
       "                (&#x27;data_scaler&#x27;, PowerTransformer()),\n",
       "                (&#x27;feature_selector&#x27;,\n",
       "                 HierarchicalFeatureSelector(n_features=50)),\n",
       "                (&#x27;data_sampler&#x27;, _RandomOverSampler()),\n",
       "                (&#x27;nn&#x27;,\n",
       "                 CombiNet(batch_size=16, build_fn=None, callbacks=None, ce_layers=10, loss=None, metrics=None, model=None, optimizer=&#x27;rmsprop&#x27;, prediction_scope={&#x27;classification&#x27;: 0, &#x27;full&#x27;: range(0, 2), &#x27;regression&#x27;: 1}, random_state=None, re_layers=10, run_eagerly=False, se_layers=2, se_units=512, shuffle=True, validation_batch_size=None, validation_split=0.0, verbose=1, warm_start=False))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VarianceThreshold</label><div class=\"sk-toggleable__content\"><pre>VarianceThreshold(threshold=10)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">PowerTransformer</label><div class=\"sk-toggleable__content\"><pre>PowerTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">HierarchicalFeatureSelector</label><div class=\"sk-toggleable__content\"><pre>HierarchicalFeatureSelector(n_features=50)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">_RandomOverSampler</label><div class=\"sk-toggleable__content\"><pre>_RandomOverSampler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CombiNet</label><div class=\"sk-toggleable__content\"><pre>CombiNet(\n",
       "\tmodel=None\n",
       "\tbuild_fn=None\n",
       "\twarm_start=False\n",
       "\trandom_state=None\n",
       "\toptimizer=rmsprop\n",
       "\tloss=None\n",
       "\tmetrics=None\n",
       "\tbatch_size=16\n",
       "\tvalidation_batch_size=None\n",
       "\tverbose=1\n",
       "\tcallbacks=None\n",
       "\tvalidation_split=0.0\n",
       "\tshuffle=True\n",
       "\trun_eagerly=False\n",
       "\tepochs=10\n",
       "\tactivation=selu\n",
       "\tse_layers=2\n",
       "\tse_units=512\n",
       "\tre_layers=10\n",
       "\tre_units=100\n",
       "\tce_layers=10\n",
       "\tce_units=100\n",
       "\tcc_units=75\n",
       "\tprediction_scope={&#x27;classification&#x27;: 0, &#x27;regression&#x27;: 1, &#x27;full&#x27;: range(0, 2)}\n",
       ")</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('variance_filter', VarianceThreshold(threshold=10)),\n",
       "                ('data_scaler', PowerTransformer()),\n",
       "                ('feature_selector',\n",
       "                 HierarchicalFeatureSelector(n_features=50)),\n",
       "                ('data_sampler', _RandomOverSampler()),\n",
       "                ('nn',\n",
       "                 CombiNet(batch_size=16, build_fn=None, callbacks=None, ce_layers=10, loss=None, metrics=None, model=None, optimizer='rmsprop', prediction_scope={'classification': 0, 'full': range(0, 2), 'regression': 1}, random_state=None, re_layers=10, run_eagerly=False, se_layers=2, se_units=512, shuffle=True, validation_batch_size=None, validation_split=0.0, verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "pipe = Pipeline([(\"variance_filter\", VarianceThreshold(10)),\n",
    "            (\"data_scaler\", PowerTransformer()),\n",
    "            (\"feature_selector\", HierarchicalFeatureSelector(n_features=50)),\n",
    "            (\"data_sampler\", _RandomOverSampler()),\n",
    "            (\"nn\", CombiNet(se_layers=2, se_units=512, batch_size=16,\n",
    "                re_layers=10, ce_layers=10, optimizer=\"rmsprop\", epochs=10, verbose=1))\n",
    "                ])\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mf/github/churn-modeling/.env/lib/python3.8/site-packages/sklearn/base.py:443: UserWarning: X has feature names, but VarianceThreshold was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 4ms/step\n",
      "f1 : 0.470\n",
      "r2 : 0.730\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, r2_score\n",
    "y_pred = pipe.predict(test.loc[:,feat_cols], scope=\"full\")\n",
    "print(\"f1 : {:.3f}\".format(f1_score(test[\"target_event\"], y_pred[:,0])))\n",
    "print(\"r2 : {:.3f}\".format(r2_score(test[\"target_cap\"], y_pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "\n",
    "class dev(CalibratedClassifierCV):\n",
    "    def __init__(*args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    @reduce_y\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        super().fit(X, y, **kwargs)\n",
    "        return self\n",
    "\n",
    "    # OVERRIDE DAT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
