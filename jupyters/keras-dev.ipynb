{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### COMBINED NEURAL NETWORK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "\n",
    "class MultiOutputTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "\n",
    "        self.class_encoder_ = OneHotEncoder(sparse=False)\n",
    "        self.reg_transformer_ = PowerTransformer()\n",
    "        # Fit them to the input data\n",
    "        self.class_encoder_.fit(y_class)\n",
    "        self.reg_transformer_.fit(y_reg)\n",
    "        # Save the number of classes\n",
    "        self.n_classes_ = len(self.class_encoder_.categories_)\n",
    "        self.n_outputs_expected_ = 2\n",
    "        return self\n",
    "\n",
    "    def transform(self, y: np.ndarray) -> List[np.ndarray]:\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "        # Apply transformers to input array\n",
    "        y_class = self.class_encoder_.transform(y_class)\n",
    "        y_reg = self.reg_transformer_.transform(y_reg)\n",
    "        # Split the data into a list\n",
    "        return [y_class, y_reg]\n",
    "\n",
    "    def inverse_transform(self, y: List[np.ndarray], return_proba: bool = False) -> np.ndarray:\n",
    "        y_pred_reg = y[1]\n",
    "        if return_proba:\n",
    "            return y[0]\n",
    "        else:\n",
    "            y_pred_class = np.zeros_like(y[0])\n",
    "            y_pred_class[np.arange(len(y[0])), np.argmax(y[0], axis=1)] = 1\n",
    "            y_pred_class = self.class_encoder_.inverse_transform(y_pred_class)\n",
    "        y_pred_reg = self.reg_transformer_.inverse_transform(y_pred_reg)\n",
    "        return np.column_stack([y_pred_class, y_pred_reg])\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return {\n",
    "            \"n_classes_\": self.n_classes_,\n",
    "            \"n_outputs_expected_\": self.n_outputs_expected_,\n",
    "        }\n",
    "\n",
    "from scikeras.wrappers import BaseWrapper\n",
    "from tensorflow.keras.initializers import HeNormal, LecunNormal, HeNormal\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, concatenate, LeakyReLU\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class CombiNet(BaseWrapper):\n",
    "\n",
    "    def __init__(self, activation = \"selu\",\n",
    "        se_layers=1, se_units=256,\n",
    "        re_layers=5, re_units=100,\n",
    "        ce_layers=5, ce_units=100, cc_units=75,\n",
    "        epochs=10, verbose=0,\n",
    "        optimizer=\"adam\", optimizer__clipvalue=1.0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.activation = activation\n",
    "            self.se_layers = se_layers\n",
    "            self.se_units = se_units\n",
    "            self.re_layers = re_layers\n",
    "            self.re_units = re_units\n",
    "            self.ce_layers = ce_layers\n",
    "            self.ce_units = ce_units\n",
    "            self.cc_units = cc_units\n",
    "            self.epochs = epochs\n",
    "            self.verbose = verbose\n",
    "            self.prediction_scope = {\"classification\":0,\"regression\":1,\"full\":range(2)}\n",
    "\n",
    "    def _get_weight_init(self):\n",
    "        if isinstance(self.activation, LeakyReLU):\n",
    "            \n",
    "            init = HeNormal()\n",
    "        elif self.activation in [\"selu\", \"elu\"]:\n",
    "            init = LecunNormal()\n",
    "        else:\n",
    "            init = HeNormal()  \n",
    "        return init\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs):\n",
    "        weight_init = self._get_weight_init()\n",
    "\n",
    "        # shared extraction\n",
    "        inp = Input(shape=(self.n_features_in_))\n",
    "        fe = inp\n",
    "        for i in range(self.se_layers):\n",
    "            fe = Dense(self.se_units, self.activation,\n",
    "                kernel_initializer=weight_init)(fe)\n",
    "            fe = BatchNormalization()(fe)\n",
    "        # regression branch\n",
    "        re = fe\n",
    "        for i in range(self.re_layers):\n",
    "            re = Dense(self.re_units, self.activation,\n",
    "                kernel_initializer=weight_init)(re)\n",
    "            re = BatchNormalization()(re)\n",
    "        rr_head = Dense(1,\"linear\")(re)\n",
    "        # classification branch\n",
    "        ce = fe\n",
    "        for i in range(self.ce_layers):\n",
    "            ce = Dense(self.ce_units, self.activation,\n",
    "                kernel_initializer=weight_init)(ce)\n",
    "            ce = BatchNormalization()(ce)\n",
    "        cc = Dense(self.cc_units, self.activation,\n",
    "            kernel_initializer=weight_init)(concatenate([ce, re]))\n",
    "        cc = BatchNormalization()(cc)\n",
    "        cc_head = Dense(2, \"softmax\")(cc)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=[cc_head, rr_head])\n",
    "        model.compile(loss=[\"categorical_crossentropy\",\"mse\"], loss_weights=[.5,.5],\n",
    "            optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model\n",
    "        \n",
    "    @property\n",
    "    def target_encoder(self):\n",
    "        return MultiOutputTransformer()\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        return self.target_encoder_.inverse_transform(y_pred, return_proba=True)\n",
    "\n",
    "    def predict(self, X, scope=\"classification\"):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        y_pred = self.target_encoder_.inverse_transform(y_pred)\n",
    "        return y_pred[:,self.prediction_scope[scope]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### FEATURE SELECTION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class DataFrameTransposer(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.copy().T\n",
    "\n",
    "class HierarchicalFeatureSelector(SelectorMixin, BaseEstimator):\n",
    "  \n",
    "    def __init__(self, n_features=10, alpha=0.001):\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha    \n",
    "\n",
    "    def _get_cluster_assignments(self, data):\n",
    "        data = data.loc[:,self.results_.feature.values]\n",
    "        n_components = data.shape[1]\n",
    "        pipe = Pipeline([(\"rotate\", DataFrameTransposer()),\n",
    "            (\"pca\", GaussianRandomProjection(n_components=n_components)),\n",
    "            (\"cluster\", AgglomerativeClustering(n_clusters=self.n_features))])\n",
    "        return pipe.fit_predict(data)\n",
    "    \n",
    "    def _get_correlations(self, X, y):\n",
    "        tf_corr = [pearsonr(y, X[c]) for c in X.columns]\n",
    "        correlations = pd.DataFrame(tf_corr, index=X.columns).reset_index()\n",
    "        correlations.columns = [\"feature\", \"r\", \"p\"]\n",
    "        correlations[\"abs_r\"] = correlations.r.abs()\n",
    "        correlations[\"sf\"] = correlations.p<=self.alpha/X.shape[1]\n",
    "        return correlations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if len(y.shape)>1:\n",
    "            y = y[:,0]\n",
    "        X = pd.DataFrame(X)\n",
    "        y = pd.Series(y)\n",
    "\n",
    "        self.in_features_ =  X.columns\n",
    "        self.results_ = self._get_correlations(X, y)\n",
    "\n",
    "        if np.sum(self.results_.sf)<= self.n_features:\n",
    "            self.best_ = self.results_[self.results_.sf]\n",
    "        else:\n",
    "            self.results_[\"cluster\"] = self._get_cluster_assignments(X)\n",
    "            self.best_ = self.results_[self.results_.sf]\\\n",
    "                .merge(self.results_.groupby(\"cluster\",\n",
    "                    as_index=False).abs_r.max(), on=[\"cluster\", \"abs_r\"])\\\n",
    "                        .drop_duplicates([\"cluster\", \"abs_r\"]).dropna()\n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        return np.array([c in set(self.best_.feature) for c in self.in_features_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### SAMPLING STRATEGIES\n",
    "# NOTE: REDO THIS USING DECORATORS\n",
    "\n",
    "from imblearn import FunctionSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "def _sampling(X, y, sampling_instance):\n",
    "    if len(y.shape)==1:\n",
    "        _ = sampling_instance.fit_resample(X, y)\n",
    "        return X[sampling_instance.sample_indices_,:], y[sampling_instance.sample_indices_]\n",
    "    else:\n",
    "        _ = sampling_instance.fit_resample(X, y[:,0])\n",
    "        return X[sampling_instance.sample_indices_,:], y[sampling_instance.sample_indices_,:]\n",
    "\n",
    "class _RandomUnderSampler(FunctionSampler):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.validate = False\n",
    "        self.func = _sampling\n",
    "        self.kw_args = {\"sampling_instance\":RandomUnderSampler()}\n",
    "\n",
    "class _RandomOverSampler(FunctionSampler):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.validate = False\n",
    "        self.func = _sampling\n",
    "        self.kw_args = {\"sampling_instance\":RandomOverSampler()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# dtype opt\n",
    "def _optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "# data load\n",
    "data = _optimize_numeric_dtypes(\n",
    "    pd.read_parquet(\"../data/customer_model/retailrocket/\"))\n",
    "train = data[data.week_step>2]\n",
    "test = data[data.week_step==2]\n",
    "\n",
    "out_cols = [\"user_id\", \"target_event\", \"target_revenue\", \"week_step\",\n",
    "    \"target_cap\"]\n",
    "feat_cols = [c for c in train.columns if c not in set(out_cols)]\n",
    "target_cols = [\"target_event\", \"target_cap\"]\n",
    "\n",
    "X = train.loc[:,feat_cols].values\n",
    "y = train.loc[:,target_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "pipe = Pipeline([(\"variance_filter\", VarianceThreshold(10)),\n",
    "            (\"data_scaler\", PowerTransformer()),\n",
    "            (\"feature_selector\", HierarchicalFeatureSelector(n_features=50)),\n",
    "            (\"data_sampler\", _RandomOverSampler()),\n",
    "            (\"nn\", CombiNet(se_layers=2, se_units=512, batch_size=16,\n",
    "                re_layers=10, ce_layers=10, optimizer=\"rmsprop\", epochs=10, verbose=1))])\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, r2_score\n",
    "y_pred = pipe.predict(test.loc[:,feat_cols], scope=\"full\")\n",
    "print(\"f1 : {:.3f}\".format(f1_score(test[\"target_event\"], y_pred[:,0])))\n",
    "print(\"r2 : {:.3f}\".format(r2_score(test[\"target_cap\"], y_pred[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALIBRATED CLASSIFIER\n",
    "    # JUST FIT ON FOLD\n",
    "    # FIT CAI\n",
    "    # OVERLOAD PREDICT AS IN COMPOSED NET \n",
    "    # HOW ABOUT JUST PREFIT AND RECALIBRATION WITHOUT THE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import warnings\n",
    "from inspect import signature\n",
    "from functools import partial\n",
    "\n",
    "from math import log\n",
    "import numpy as np\n",
    "from joblib import Parallel\n",
    "\n",
    "from scipy.special import expit\n",
    "from scipy.special import xlogy\n",
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "from sklearn.base import (\n",
    "    BaseEstimator,\n",
    "    ClassifierMixin,\n",
    "    RegressorMixin,\n",
    "    clone,\n",
    "    MetaEstimatorMixin,\n",
    "    is_classifier,\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize, LabelEncoder\n",
    "from sklearn.utils import (\n",
    "    column_or_1d,\n",
    "    indexable,\n",
    "    check_matplotlib_support,\n",
    ")\n",
    "\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.utils.fixes import delayed\n",
    "from sklearn.utils.validation import (\n",
    "    _check_fit_params,\n",
    "    _check_sample_weight,\n",
    "    _num_samples,\n",
    "    check_consistent_length,\n",
    "    check_is_fitted,\n",
    ")\n",
    "from sklearn.utils import _safe_indexing\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import check_cv, cross_val_predict\n",
    "from sklearn.metrics._base import _check_pos_label_consistency\n",
    "from sklearn.metrics._plot.base import _get_response\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "class DEVCC(CalibratedClassifierCV):\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "            \n",
    "            self.y_ = copy.copy(y)\n",
    "            if len(y.shape)>1:\n",
    "                y = y[:,0]\n",
    "            \n",
    "            check_classification_targets(y)\n",
    "            X, y = indexable(X, y)\n",
    "            if sample_weight is not None:\n",
    "                sample_weight = _check_sample_weight(sample_weight, X)\n",
    "\n",
    "            for sample_aligned_params in fit_params.values():\n",
    "                check_consistent_length(y, sample_aligned_params)\n",
    "\n",
    "            if self.base_estimator is None:\n",
    "                # we want all classifiers that don't expose a random_state\n",
    "                # to be deterministic (and we don't want to expose this one).\n",
    "                base_estimator = LinearSVC(random_state=0)\n",
    "            else:\n",
    "                base_estimator = self.base_estimator\n",
    "\n",
    "            self.calibrated_classifiers_ = []\n",
    "            if self.cv == \"prefit\":\n",
    "                # `classes_` should be consistent with that of base_estimator\n",
    "                check_is_fitted(self.base_estimator, attributes=[\"classes_\"])\n",
    "                self.classes_ = self.base_estimator.classes_\n",
    "\n",
    "                pred_method, method_name = _get_prediction_method(base_estimator)\n",
    "                n_classes = len(self.classes_)\n",
    "                predictions = _compute_predictions(pred_method, method_name, X, n_classes)\n",
    "\n",
    "                calibrated_classifier = _fit_calibrator(\n",
    "                    base_estimator,\n",
    "                    predictions,\n",
    "                    y,\n",
    "                    self.classes_,\n",
    "                    self.method,\n",
    "                    sample_weight,\n",
    "                )\n",
    "                self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "            else:\n",
    "                # Set `classes_` using all `y`\n",
    "                label_encoder_ = LabelEncoder().fit(y)\n",
    "                self.classes_ = label_encoder_.classes_\n",
    "                n_classes = len(self.classes_)\n",
    "\n",
    "                # sample_weight checks\n",
    "                fit_parameters = signature(base_estimator.fit).parameters\n",
    "                supports_sw = \"sample_weight\" in fit_parameters\n",
    "                if sample_weight is not None and not supports_sw:\n",
    "                    estimator_name = type(base_estimator).__name__\n",
    "                    warnings.warn(\n",
    "                        f\"Since {estimator_name} does not appear to accept sample_weight, \"\n",
    "                        \"sample weights will only be used for the calibration itself. This \"\n",
    "                        \"can be caused by a limitation of the current scikit-learn API. \"\n",
    "                        \"See the following issue for more details: \"\n",
    "                        \"https://github.com/scikit-learn/scikit-learn/issues/21134. Be \"\n",
    "                        \"warned that the result of the calibration is likely to be \"\n",
    "                        \"incorrect.\"\n",
    "                    )\n",
    "\n",
    "                # Check that each cross-validation fold can have at least one\n",
    "                # example per class\n",
    "                if isinstance(self.cv, int):\n",
    "                    n_folds = self.cv\n",
    "                elif hasattr(self.cv, \"n_splits\"):\n",
    "                    n_folds = self.cv.n_splits\n",
    "                else:\n",
    "                    n_folds = None\n",
    "                if n_folds and np.any(\n",
    "                    [np.sum(y == class_) < n_folds for class_ in self.classes_]\n",
    "                ):\n",
    "                    raise ValueError(\n",
    "                        f\"Requesting {n_folds}-fold \"\n",
    "                        \"cross-validation but provided less than \"\n",
    "                        f\"{n_folds} examples for at least one class.\"\n",
    "                    )\n",
    "                cv = check_cv(self.cv, y, classifier=True)\n",
    "\n",
    "                if self.ensemble:\n",
    "                    parallel = Parallel(n_jobs=self.n_jobs)\n",
    "                    self.calibrated_classifiers_ = parallel(\n",
    "                        delayed(_fit_classifier_calibrator_pair)(\n",
    "                            clone(base_estimator),\n",
    "                            X,\n",
    "                            y,\n",
    "                            train=train,\n",
    "                            test=test,\n",
    "                            method=self.method,\n",
    "                            classes=self.classes_,\n",
    "                            supports_sw=supports_sw,\n",
    "                            sample_weight=sample_weight,\n",
    "                            **fit_params,\n",
    "                        )\n",
    "                        for train, test in cv.split(X, y)\n",
    "                    )\n",
    "                else:\n",
    "                    this_estimator = clone(base_estimator)\n",
    "                    _, method_name = _get_prediction_method(this_estimator)\n",
    "                    fit_params = (\n",
    "                        {\"sample_weight\": sample_weight}\n",
    "                        if sample_weight is not None and supports_sw\n",
    "                        else None\n",
    "                    )\n",
    "                    pred_method = partial(\n",
    "                        cross_val_predict,\n",
    "                        estimator=this_estimator,\n",
    "                        X=X,\n",
    "                        y=y,\n",
    "                        cv=cv,\n",
    "                        method=method_name,\n",
    "                        n_jobs=self.n_jobs,\n",
    "                        fit_params=fit_params,\n",
    "                    )\n",
    "                    predictions = _compute_predictions(\n",
    "                        pred_method, method_name, X, n_classes\n",
    "                    )\n",
    "\n",
    "                    if sample_weight is not None and supports_sw:\n",
    "                        this_estimator.fit(X, y, sample_weight=sample_weight)\n",
    "                    else:\n",
    "                        this_estimator.fit(X, y)\n",
    "                    # Note: Here we don't pass on fit_params because the supported\n",
    "                    # calibrators don't support fit_params anyway\n",
    "                    calibrated_classifier = _fit_calibrator(\n",
    "                        this_estimator,\n",
    "                        predictions,\n",
    "                        y,\n",
    "                        self.classes_,\n",
    "                        self.method,\n",
    "                        sample_weight,\n",
    "                    )\n",
    "                    self.calibrated_classifiers_.append(calibrated_classifier)\n",
    "\n",
    "            first_clf = self.calibrated_classifiers_[0].base_estimator\n",
    "            if hasattr(first_clf, \"n_features_in_\"):\n",
    "                self.n_features_in_ = first_clf.n_features_in_\n",
    "            if hasattr(first_clf, \"feature_names_in_\"):\n",
    "                self.feature_names_in_ = first_clf.feature_names_in_\n",
    "            return self\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def reduce_y(func):\n",
    "    def actual_reduce(self, y, *args, **kwargs):\n",
    "        if len(y.shape)>1:\n",
    "            self.y_ = copy(y)\n",
    "            y = y[:,0]\n",
    "        func(self, y, *args, **kwargs)\n",
    "    return actual_reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVCC(pipe).method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
