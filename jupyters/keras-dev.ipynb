{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### COMBINED NEURAL NETWORK\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "\n",
    "class MultiOutputTransformer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def fit(self, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "\n",
    "        self.class_encoder_ = OneHotEncoder(sparse=False)\n",
    "        self.reg_transformer_ = PowerTransformer()\n",
    "        # Fit them to the input data\n",
    "        self.class_encoder_.fit(y_class)\n",
    "        self.reg_transformer_.fit(y_reg)\n",
    "        # Save the number of classes\n",
    "        self.n_classes_ = len(self.class_encoder_.categories_)\n",
    "        self.n_outputs_expected_ = 2\n",
    "        return self\n",
    "\n",
    "    def transform(self, y):\n",
    "        if isinstance(y, pd.DataFrame):\n",
    "            y = y.values\n",
    "        y_class, y_reg = y[:, 0].reshape(-1,1), y[:, 1].reshape(-1,1)\n",
    "        # Apply transformers to input array\n",
    "        y_class = self.class_encoder_.transform(y_class)\n",
    "        y_reg = self.reg_transformer_.transform(y_reg)\n",
    "        # Split the data into a list\n",
    "        return [y_class, y_reg]\n",
    "\n",
    "    def inverse_transform(self, y, return_proba=False):\n",
    "        y_pred_reg = y[1]\n",
    "        if return_proba:\n",
    "            return y[0]\n",
    "        else:\n",
    "            y_pred_class = np.zeros_like(y[0])\n",
    "            y_pred_class[np.arange(len(y[0])), np.argmax(y[0], axis=1)] = 1\n",
    "            y_pred_class = self.class_encoder_.inverse_transform(y_pred_class)\n",
    "        y_pred_reg = self.reg_transformer_.inverse_transform(y_pred_reg)\n",
    "        return np.column_stack([y_pred_class, y_pred_reg])\n",
    "\n",
    "    def get_metadata(self):\n",
    "        return {\n",
    "            \"n_classes_\": self.n_classes_,\n",
    "            \"n_outputs_expected_\": self.n_outputs_expected_,\n",
    "        }\n",
    "\n",
    "from scikeras.wrappers import BaseWrapper\n",
    "from tensorflow.keras.initializers import HeNormal, LecunNormal, HeNormal\n",
    "from tensorflow.keras.layers import Input, Dense, BatchNormalization, concatenate, LeakyReLU\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class CombiNet(BaseWrapper):\n",
    "\n",
    "    def __init__(self, activation = \"selu\",\n",
    "        se_layers=1, se_units=256,\n",
    "        re_layers=5, re_units=100,\n",
    "        ce_layers=5, ce_units=100, cc_units=75,\n",
    "        epochs=10, verbose=0,\n",
    "        optimizer=\"adam\", optimizer__clipvalue=1.0, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.activation = activation\n",
    "            self.se_layers = se_layers\n",
    "            self.se_units = se_units\n",
    "            self.re_layers = re_layers\n",
    "            self.re_units = re_units\n",
    "            self.ce_layers = ce_layers\n",
    "            self.ce_units = ce_units\n",
    "            self.cc_units = cc_units\n",
    "            self.epochs = epochs\n",
    "            self.verbose = verbose\n",
    "            self.prediction_scope_ = {\"classification\":0,\"regression\":1,\"full\":range(2)}\n",
    "\n",
    "    def _get_weight_init(self):\n",
    "        if isinstance(self.activation, LeakyReLU):\n",
    "            \n",
    "            init = HeNormal()\n",
    "        elif self.activation in [\"selu\", \"elu\"]:\n",
    "            init = LecunNormal()\n",
    "        else:\n",
    "            init = HeNormal()  \n",
    "        return init\n",
    "\n",
    "    def _keras_build_fn(self, compile_kwargs):\n",
    "        weight_init = self._get_weight_init()\n",
    "\n",
    "        # shared extraction\n",
    "        inp = Input(shape=(self.n_features_in_))\n",
    "        fe = inp\n",
    "        for i in range(self.se_layers):\n",
    "            fe = Dense(self.se_units, self.activation,\n",
    "                kernel_initializer=weight_init)(fe)\n",
    "            fe = BatchNormalization()(fe)\n",
    "        # regression branch\n",
    "        re = fe\n",
    "        for i in range(self.re_layers):\n",
    "            re = Dense(self.re_units, self.activation,\n",
    "                kernel_initializer=weight_init)(re)\n",
    "            re = BatchNormalization()(re)\n",
    "        rr_head = Dense(1,\"linear\")(re)\n",
    "        # classification branch\n",
    "        ce = fe\n",
    "        for i in range(self.ce_layers):\n",
    "            ce = Dense(self.ce_units, self.activation,\n",
    "                kernel_initializer=weight_init)(ce)\n",
    "            ce = BatchNormalization()(ce)\n",
    "        cc = Dense(self.cc_units, self.activation,\n",
    "            kernel_initializer=weight_init)(concatenate([ce, re]))\n",
    "        cc = BatchNormalization()(cc)\n",
    "        cc_head = Dense(2, \"softmax\")(cc)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=[cc_head, rr_head])\n",
    "        model.compile(loss=[\"categorical_crossentropy\",\"mse\"], loss_weights=[.5,.5],\n",
    "            optimizer=compile_kwargs[\"optimizer\"])\n",
    "        return model\n",
    "        \n",
    "    @property\n",
    "    def target_encoder(self):\n",
    "        return MultiOutputTransformer()\n",
    "        \n",
    "    def predict_proba(self, X):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        return self.target_encoder_.inverse_transform(y_pred, return_proba=True)\n",
    "\n",
    "    def predict(self, X, scope=\"classification\"):\n",
    "        X = self.feature_encoder_.transform(X)\n",
    "        y_pred = self.model_.predict(X)\n",
    "        y_pred = self.target_encoder_.inverse_transform(y_pred)\n",
    "        return y_pred[:,self.prediction_scope_[scope]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def reduce_y(func):\n",
    "    def actual_reduce(self, X, y,  *args, **kwargs):\n",
    "        if self.y_ is None:\n",
    "            self.y_ = copy.copy(y)\n",
    "        if len(y.shape)>1:\n",
    "            y = y[:,0]\n",
    "        return func(self, X, y, *args, **kwargs)\n",
    "    return actual_reduce\n",
    "\n",
    "def expand_y(func):\n",
    "    def actual_expand(self, X, y, *args, **kwargs):\n",
    "        #if len(self.y_.shape)>1:\n",
    "        y = copy.copy(self.y_)\n",
    "        return func(self, X, y, *args, **kwargs)\n",
    "    return actual_expand    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### FEATURE SELECTION\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_selection import SelectorMixin\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "\n",
    "class DataFrameTransposer(BaseEstimator, TransformerMixin):\n",
    "  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.copy().T\n",
    "\n",
    "class HierarchicalFeatureSelector(SelectorMixin, BaseEstimator):\n",
    "  \n",
    "    def __init__(self, n_features=10, alpha=0.001):\n",
    "        self.n_features = n_features\n",
    "        self.alpha = alpha\n",
    "        self.y_=None    \n",
    "\n",
    "    def _get_cluster_assignments(self, data):\n",
    "        data = data.loc[:,self.results_.feature.values]\n",
    "        n_components = data.shape[1]\n",
    "        pipe = Pipeline([(\"rotate\", DataFrameTransposer()),\n",
    "            (\"pca\", GaussianRandomProjection(n_components=n_components)),\n",
    "            (\"cluster\", AgglomerativeClustering(n_clusters=self.n_features))])\n",
    "        return pipe.fit_predict(data)\n",
    "    \n",
    "    @reduce_y\n",
    "    def _get_correlations(self, X, y):\n",
    "        tf_corr = [pearsonr(y, X[c]) for c in X.columns]\n",
    "        correlations = pd.DataFrame(tf_corr, index=X.columns).reset_index()\n",
    "        correlations.columns = [\"feature\", \"r\", \"p\"]\n",
    "        correlations[\"abs_r\"] = correlations.r.abs()\n",
    "        correlations[\"sf\"] = correlations.p<=self.alpha/X.shape[1]\n",
    "        return correlations\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        #if len(y.shape)>1:\n",
    "        #    y = y[:,0]\n",
    "        X = pd.DataFrame(X)\n",
    "        #y = pd.Series(y)\n",
    "\n",
    "        self.in_features_ =  X.columns\n",
    "        self.results_ = self._get_correlations(X, y)\n",
    "\n",
    "        if np.sum(self.results_.sf)<= self.n_features:\n",
    "            self.best_ = self.results_[self.results_.sf]\n",
    "        else:\n",
    "            self.results_[\"cluster\"] = self._get_cluster_assignments(X)\n",
    "            self.best_ = self.results_[self.results_.sf]\\\n",
    "                .merge(self.results_.groupby(\"cluster\",\n",
    "                    as_index=False).abs_r.max(), on=[\"cluster\", \"abs_r\"])\\\n",
    "                        .drop_duplicates([\"cluster\", \"abs_r\"]).dropna()\n",
    "        return self\n",
    "    \n",
    "    def _get_support_mask(self):\n",
    "        return np.array([c in set(self.best_.feature) for c in self.in_features_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "##\n",
    "### SAMPLING STRATEGIES\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "class _RandomUnderSampler(RandomUnderSampler):\n",
    "    def __init__(self, sampling_strategy=\"auto\"):\n",
    "        super().__init__(sampling_strategy)\n",
    "        self.y_=None\n",
    "    @reduce_y\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        super().fit(X, y, **kwargs)\n",
    "        return self\n",
    "    @reduce_y\n",
    "    def fit_resample(self, X, y, **kwargs):\n",
    "        _ = super().fit_resample(X, y, **kwargs)\n",
    "        ind = self.sample_indices_\n",
    "        return X[ind,:], self.y_[ind]\n",
    "\n",
    "class _RandomOverSampler(RandomOverSampler):\n",
    "    def __init__(self, sampling_strategy=\"auto\"):\n",
    "        super().__init__(sampling_strategy)\n",
    "        self.y_=None\n",
    "    @reduce_y\n",
    "    def fit(self, X, y, **kwargs):\n",
    "        super().fit(X, y, **kwargs)\n",
    "        return self\n",
    "    @reduce_y\n",
    "    def fit_resample(self, X, y, **kwargs):\n",
    "        _ = super().fit_resample(X, y, **kwargs)\n",
    "        ind = self.sample_indices_\n",
    "        return X[ind,:], self.y_[ind]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# dtype opt\n",
    "def _optimize_numeric_dtypes(df):\n",
    "    import pandas as pd\n",
    "    float_cols = df.select_dtypes(\"float\").columns\n",
    "    int_cols = df.select_dtypes(\"integer\").columns\n",
    "    df[float_cols] = df[float_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"float\")\n",
    "    df[int_cols] = df[int_cols].\\\n",
    "        apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "# data load\n",
    "data = _optimize_numeric_dtypes(\n",
    "    pd.read_parquet(\"../data/customer_model/retailrocket/\"))\n",
    "train = data[data.week_step>2]\n",
    "test = data[data.week_step==2]\n",
    "\n",
    "out_cols = [\"user_id\", \"target_event\", \"target_revenue\", \"week_step\",\n",
    "    \"target_cap\"]\n",
    "feat_cols = [c for c in train.columns if c not in set(out_cols)]\n",
    "target_cols = [\"target_event\", \"target_cap\"]\n",
    "\n",
    "X = train.loc[:,feat_cols].values\n",
    "y = train.loc[:,target_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "pipe = Pipeline([(\"variance_filter\", VarianceThreshold(10)),\n",
    "            (\"data_scaler\", PowerTransformer()),\n",
    "            (\"feature_selector\", HierarchicalFeatureSelector(n_features=50)),\n",
    "            (\"data_sampler\", _RandomOverSampler()),\n",
    "            (\"nn\", CombiNet(se_layers=2, se_units=512, batch_size=16,\n",
    "                re_layers=10, ce_layers=10, optimizer=\"rmsprop\", epochs=5, verbose=1))\n",
    "                ])\n",
    "pipe.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, r2_score\n",
    "y_pred = pipe.predict(test.loc[:,feat_cols], scope=\"full\")\n",
    "print(\"f1 : {:.3f}\".format(f1_score(test[\"target_event\"], y_pred[:,0])))\n",
    "print(\"r2 : {:.3f}\".format(r2_score(test[\"target_cap\"], y_pred[:,1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0302efd4727f45c27e6e7330619db7bcf8ae8a56f076c44e120407f8390c5d65"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
